{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f87f3-03fd-4d41-b646-282a3d3e5b2f",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "- Time to First Token (TTFT): latency before the first output token is produced.\n",
    "- End-to-End Request Latency : how long it takes from submitting a query to receiving the full response\n",
    "- Time Per Output Token (TPOT): average generation speed in tokens per second. (also known as Inter-token Latency (ITL))\n",
    "- Token Generation Time (TGT): duration from first to last token.\n",
    "- Total Latency: TTFT + TGT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d997d1f-0968-4186-8a5d-e0aa310d3ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loosely based on code acquired from https://github.com/rumanxyz/llm-perf-benchmark\n",
    "import torch\n",
    "import time\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import traceback\n",
    "import threading\n",
    "from transformers import TextIteratorStreamer\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "\n",
    "\n",
    "class GPUMonitor:\n",
    "    def __init__(self, monitoring_interval: float = 0.1, gpu_indices: Optional[List[int]] = None):\n",
    "        self.monitoring_interval = monitoring_interval\n",
    "        self.gpu_indices = gpu_indices\n",
    "        self._mem_samples = []\n",
    "        self._util_samples = []\n",
    "        self._is_monitoring = False\n",
    "        self._monitoring_thread = None\n",
    "\n",
    "    def start(self):\n",
    "        self._is_monitoring = True\n",
    "        self._mem_samples.clear()\n",
    "        self._util_samples.clear()\n",
    "\n",
    "        def monitor_gpu():\n",
    "            while self._is_monitoring:\n",
    "                try:\n",
    "                    gpus = GPUtil.getGPUs()\n",
    "                    if gpus:\n",
    "                        idxs = self.gpu_indices if self.gpu_indices is not None else list(range(len(gpus)))\n",
    "                        mem_mb = 0.0\n",
    "                        util_pct = 0.0\n",
    "\n",
    "                        for i in idxs:\n",
    "                            gpu = gpus[i]\n",
    "                            mem_mb += self._gpu_memory_usage.append(gpu.memoryUsed)\n",
    "                            util_pct += self._gpu_utilization.append(gpu.load * 100)\n",
    "\n",
    "                        self._mem_samples.append(mem_mb)\n",
    "                        self._util_samples.append(util_pct)\n",
    "                    time.sleep(self.monitoring_interval)\n",
    "                except Exception as e:\n",
    "                    print(f\"GPU monitoring error: {e}\")\n",
    "                    break\n",
    "\n",
    "        self._monitoring_thread = threading.Thread(target=monitor_gpu, daemon=True)\n",
    "        self._monitoring_thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop GPU monitoring\"\"\"\n",
    "        self._is_monitoring = False\n",
    "        if self._monitoring_thread:\n",
    "            self._monitoring_thread.join()\n",
    "\n",
    "    def peak_mem(self) -> float:\n",
    "        return max(self._mem_samples) if self._mem_samples else 0.0\n",
    "\n",
    "    def p90_mem(self) -> float:\n",
    "        return float(np.percentile(self._mem_samples, 90)) if self._mem_samples else 0.0\n",
    "\n",
    "    def peak_util(self) -> float:\n",
    "        return max(self._util_samples) if self._util_samples else 0.0\n",
    "\n",
    "    def p90_util(self) -> float:\n",
    "        return float(np.percentile(self._util_samples, 90)) if self._util_samples else 0.0\n",
    "\n",
    "\n",
    "def benchmark_single_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_prompt_text: str,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    max_new_tokens: int = 100,\n",
    "    device: Optional[str] = None,\n",
    "    gpu_indices: Optional[List[int]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmark a language model's performance for a single prompt.\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        try:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # GPU monitoring setup\n",
    "    gpu_monitor = GPUMonitor(monitoring_interval=0.1, gpu_indices=gpu_indices)\n",
    "    gpu_monitor.start()\n",
    "\n",
    "    # Tokenize input\n",
    "    start_input_process = time.time()\n",
    "    inputs = tokenizer(input_prompt_text, return_tensors=\"pt\").to(device)\n",
    "    input_process_time = time.time() - start_input_process\n",
    "\n",
    "    generation_kwargs = {\n",
    "        'input_ids': inputs.input_ids,\n",
    "        'attention_mask': inputs.attention_mask,\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'do_sample': temperature is not None and temperature > 0,\n",
    "        'return_dict_in_generate': True,\n",
    "        'output_scores': False\n",
    "    }\n",
    "\n",
    "    # Streaming generation setup\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=False)\n",
    "    gen_kwargs[\"streamer\"] = streamer\n",
    "\n",
    "    generation_start_time = time.time()\n",
    "    first_token_time = None\n",
    "    first_token_wall = None\n",
    "    result_holder = {}\n",
    "\n",
    "    def _generate():\n",
    "        result_holder[\"out\"] = model.generate(**gen_kwargs)\n",
    "\n",
    "    generation_thread = threading.Thread(target=generate, daemon=True)\n",
    "    generation_thread.start()\n",
    "\n",
    "    # Streaming generation loop\n",
    "    try:\n",
    "        for token in streamer:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - generation_start_time\n",
    "                first_token_start_time = time.time()\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        print(f\"Error trace:\\n{traceback.format_exc()}\")\n",
    "        gpu_monitor.stop()\n",
    "        return {}\n",
    "\n",
    "    # Stop GPU monitoring\n",
    "    generation_thread.join()\n",
    "    gpu_monitor.stop()\n",
    "\n",
    "    if \"out\" not in result_holder:\n",
    "        return {}\n",
    "\n",
    "    output = result_holder[\"out\"]\n",
    "    sequences = output.sequences\n",
    "\n",
    "    total_len = int(sequences.shape[1])\n",
    "    input_tokens = int(enc.input_ids.shape[1])\n",
    "    output_tokens = max(total_len - input_tokens, 0)\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "\n",
    "    total_generation_time = time.time() - generation_start_time\n",
    "\n",
    "    if first_token_wall is not None:\n",
    "        decode_time = max(time.time() - first_token_wall, 1e-9)\n",
    "        ttft = first_token_time\n",
    "    else:\n",
    "        decode_time = total_time\n",
    "        ttft = None\n",
    "\n",
    "    # Metrics\n",
    "    total_tps = (input_tokens + output_tokens) / max(total_time, 1e-9)\n",
    "    decode_tps = (output_tokens) / max(decode_time, 1e-9)\n",
    "\n",
    "    # GPU metrics\n",
    "    peak_gpu_usage = mon.peak_mem()\n",
    "    p90_gpu_usage = mon.p90_mem()\n",
    "    peak_gpu_utilization = mon.peak_util()\n",
    "    p90_gpu_utilization = mon.p90_util()\n",
    "\n",
    "    benchmark_results = {\n",
    "        'total_generation_time': total_generation_time,\n",
    "        'time_to_first_token_seconds': ttft,\n",
    "        'token_generation_time' : decode_time,\n",
    "        'time_per_output_token' : 1 / decode_tps,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'tokens_per_second': total_tps,\n",
    "        'output_decode_tokens_per_second': decode_tps,\n",
    "        'input_process_time_seconds': input_process_time,\n",
    "        'e2e_latency' : ttft + total_generation_time, \n",
    "        'peak_gpu_memory_mb': peak_gpu_usage,\n",
    "        'p90_gpu_memory_mb': p90_gpu_usage,\n",
    "        'peak_gpu_utilization': peak_gpu_utilization,\n",
    "        'p90_gpu_utilization': p90_gpu_utilization\n",
    "    }\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "def benchmark_language_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    max_new_tokens: int = 100,\n",
    "    device: Optional[str] = None,\n",
    "    gpu_indices: Optional[List[int]] = None,\n",
    ") -> Dict[str, Union[float, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Benchmark a language model's performance across multiple prompts.\n",
    "    \"\"\"\n",
    "    prompt_results = []\n",
    "    for prompt in prompts:\n",
    "        result = benchmark_single_prompt(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            max_new_tokens,\n",
    "            device\n",
    "        )\n",
    "        if result:\n",
    "            prompt_results.append(result)\n",
    "\n",
    "    if not prompt_results:\n",
    "        return {}\n",
    "\n",
    "    # Extract metric lists for aggregation\n",
    "    ttft_list = [result['time_to_first_token_seconds'] for result in prompt_results]\n",
    "    tpot_list = [result['time_per_output_token'] for result in prompt_results]\n",
    "    tgt_list = [result['total_generation_time'] for result in prompt_results]\n",
    "    e2e_latency_list = [result['e2e_latency'] for result in prompt_results]\n",
    "    decode_tps_list = [result['output_decode_tokens_per_second'] for result in prompt_results]\n",
    "    gpu_usage_list = [result['peak_gpu_memory_mb'] for result in prompt_results]\n",
    "    gpu_util_list = [result['peak_gpu_utilization'] for result in prompt_results]\n",
    "\n",
    "    # Aggregate metrics\n",
    "    aggregate_results = {\n",
    "        # Time to First Token (TTFT) metrics\n",
    "        'p50_ttft_seconds': round(np.percentile(ttft_list, 50), 3),\n",
    "        'p90_ttft_seconds': round(np.percentile(ttft_list, 90), 3),\n",
    "\n",
    "        # Time per output token (TPOT) metrics\n",
    "        'p50_tpot_seconds': round(np.percentile(tpot_list, 50), 3),\n",
    "        'p90_tpot_seconds': round(np.percentile(tpot_list, 90), 3),\n",
    "        \n",
    "        # Total generation time (TGT) metrics\n",
    "        'p50_tgt_seconds': round(np.percentile(tgt_list, 50), 3),\n",
    "        'p90_tgt_seconds': round(np.percentile(tgt_list, 90), 3),\n",
    "        \n",
    "        # End to end latency (e2e latency) metrics\n",
    "        'p50_e2elatency_seconds': round(np.percentile(e2e_latency_list, 50), 3),\n",
    "        'p90_e2elatency_seconds': round(np.percentile(e2e_latency_list, 90), 3),\n",
    "\n",
    "        # Output Decode Tokens Per Second metrics\n",
    "        'p50_decode_tps': round(np.percentile(decode_tps_list, 50), 3),\n",
    "        'p90_decode_tps': round(np.percentile(decode_tps_list, 90), 3),\n",
    "\n",
    "        # GPU Memory Usage metrics\n",
    "        'max_gpu_memory_mb': round(max(gpu_usage_list), 3),\n",
    "        'p90_gpu_memory_mb': round(np.percentile(gpu_usage_list, 90), 3),\n",
    "\n",
    "        # GPU Utilization metrics\n",
    "        'max_gpu_utilization': round(max(gpu_util_list), 3),\n",
    "        'p90_gpu_utilization': round(np.percentile(gpu_util_list, 90), 3)\n",
    "    }\n",
    "\n",
    "    return aggregate_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
