{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cbIaeP9pX07"
   },
   "source": [
    "# Natural Language Processing - Assignment 2\n",
    "# Sentiment analysis for movie reviews\n",
    "\n",
    "This notebook was created for you to answer question 2, 3 and 4 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. \n",
    "\n",
    "The (red) comments at the beginning of each function explain what they should do, which parameters you should give as input and which variables should be returned by the function. After the (green) comments \"### student code here###' you should write your own code.\n",
    "\n",
    "**Please modify the next cell specifying your group number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hIJFbDA1Qbi"
   },
   "source": [
    " *This is the Notebook of* ***Group 30*** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQfxb4pUNs1-"
   },
   "source": [
    "### Prerequisite - Libraries\n",
    "Make sure you have the needed libraries installed on your computer: scikit-learn, Pandas, NLTK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KiI6RyOpX08"
   },
   "source": [
    "### Prerequisite - Load Data\n",
    "\n",
    "In the first step, we are going to load the data in a Pandas DataFrame. Pandas DataFrames are a useful way of storing data. DataFrames¬†are tables in which data can be accessed as columns, as rows or as individual cells. You can find more info on DataFrames here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "Read the code below and make sure you understand what is happening. Run the code to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hX1AE_fJpX09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Marko\n",
      "[nltk_data]     Haraloviƒá\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Marko\n",
      "[nltk_data]     Haraloviƒá\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "### student code here: import the needed modules from sci-kit learn ###\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountVectorizerOff\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eazU-uYcpX1B"
   },
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "    # if you are using Google Colab, you will have to change the above line\n",
    "    # to load the dataset¬†from your Google Drive\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/movies/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/movies, but could not find it.')\n",
    "    else: \n",
    "        print(\"Found \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RrcOjEdSpX1E"
   },
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-(train|test)[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cvGgLWN_pX1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  1200 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...</td>\n",
       "      <td>This is a pale imitation of 'Officer and a Gen...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...</td>\n",
       "      <td>It seems ever since 1982, about every two or t...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...</td>\n",
       "      <td>Wow, another Kevin Costner hero movie. Postman...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...</td>\n",
       "      <td>Alas, another Costner movie that was an hour t...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  \\\n",
       "0  c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...   \n",
       "1  c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...   \n",
       "2  c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...   \n",
       "3  c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...   \n",
       "4  c:\\FER\\9TH SEMESTER\\NLP\\homeworks\\hw2/movies/t...   \n",
       "\n",
       "                                              Review Label  \n",
       "0  Once again Mr. Costner has dragged out a movie...   Neg  \n",
       "1  This is a pale imitation of 'Officer and a Gen...   Neg  \n",
       "2  It seems ever since 1982, about every two or t...   Neg  \n",
       "3  Wow, another Kevin Costner hero movie. Postman...   Neg  \n",
       "4  Alas, another Costner movie that was an hour t...   Neg  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the files in the Dataframe. This will take a while...\n",
    "paths = get_path('train/[NP]-train[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "### Part 2 - Tokenization\n",
    "\n",
    "In this step, you should write a tokenizer and compare it with an off-the-shelf one.\n",
    "\n",
    "#### Question 2.1 Making your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UkZwy1ATNs2F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'have', 'the', 'chance', ',', 'watch', 'it', '.', 'although', ',', 'a', 'warning', ',', 'you', 'll', 'cry', 'your', 'eye', 'out']\n",
      "['i', 'don', 't', 'know', 'what', 'to', 'do', 'with', 'this', 'course', 'i', 've', 'taken', ',', 'it', 'ha', 'all', 'these', 'assignment']\n",
      "['i', 'tried', 'to', 'like', 'this', 'movie', ',', 'i', 'really', 'did', ',', 'but', 'it', 'wa', 'just', 'so', 'bad']\n",
      "['no', 'matter', 'what', 'i', 'do', ',', 'my', 'girlfriend', 'is', 'always', 'right', '.', 'i', 'decided', 'to', 'acccept', 'it']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    The implementation of your own tokenizer\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"    \n",
    "    ### student code here ###\n",
    "    \n",
    "    # part 1: regex to find the words\n",
    "    regex = r'[a-zA-Z]+(?:\\.[a-zA-Z]+)*|\\d+|[.!?,:;()\"\\'](?=\\s)'\n",
    "    words = re.findall(regex, text.lower())\n",
    "    \n",
    "    # part 2: lemmatization\n",
    "    lematized_words = [WordNetLemmatizer().lemmatize(word) if word.isalpha() else word for word  in words]\n",
    "\n",
    "    tokenized_text = lematized_words\n",
    "    return tokenized_text\n",
    "\n",
    "sample_string0 = \"If you have the chance, watch it. Although, a warning, you'll cry your eyes out.\"\n",
    "sample_string1 = \"I don't know what to do with this course I've taken, it has all these assignments...\"\n",
    "sample_string2 = \"I tried to like this movie, I really did, but it was just so bad.\" \n",
    "sample_string3 = \"No matter what I do, my girlfriend is always right. I decided to acccept it\"\n",
    "print(my_tokenizer(sample_string0))\n",
    "print(my_tokenizer(sample_string1))\n",
    "print(my_tokenizer(sample_string2))\n",
    "print(my_tokenizer(sample_string3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pxI0gdoNs2G"
   },
   "source": [
    "#### Question 2.2 Using an off-the-shelf tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TXUTKVyqNs2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'this', 'assignment', 'because', ':', 'it', 'is', 'fun', ';', 'it', 'help', 'me', 'practice', 'my', 'python', 'skill']\n",
      "['i', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'python', 'skills', '.']\n",
      "\n",
      "\n",
      "['i', 'won', 'a', 'prize', ',', 'but', 'i', 'won', 't', 'be', 'able', 'to', 'attend', 'the', 'ceremony']\n",
      "['i', 'won', 'a', 'prize', ',', 'but', 'i', 'wo', \"n't\", 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n",
      "\n",
      "\n",
      "['the', 'strange', 'case', 'of', 'dr', '.', 'jekyll', 'and', 'mr', '.', 'hyde', 'is', 'a', 'famous', 'book', '.', 'but', 'i', 'haven', 't', 'read', 'it']\n",
      "['‚Äú', 'the', 'strange', 'case', 'of', 'dr.', 'jekyll', 'and', 'mr.', 'hyde', '‚Äù', 'is', 'a', 'famous', 'book', '...', 'but', 'i', 'have', \"n't\", 'read', 'it', '.']\n",
      "\n",
      "\n",
      "['i', 'work', 'for', 'the', 'c.i.a', '.', 'and', 'you']\n",
      "['i', 'work', 'for', 'the', 'c.i.a', '..', 'and', 'you', '?']\n",
      "\n",
      "\n",
      "['omg', 'twitter', 'is', 'sooooo', 'coooool', '3', ')', 'lol', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ')']\n",
      "['omg', '#', 'twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '--', 'lol', '...', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', 'ü§∑üòÇ', 'ü§ñ']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we are gonna compare the tokenizer you just wrote with the one from NLTK\n",
    "#if you installed NLTK but never downloaded the 'punkt' tokenizer, uncomment the following lines:\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function should apply the word_tokenize (punkt) tokenizer of nltk to the input text\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"     \n",
    "    ### student code here ###    \n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "test_sentences = [\"I like this assignment because:\\n-\\tit is fun;\\n-\\tit helps me practice my Python skills.\",\n",
    "        \"I won a prize, but I won't be able to attend the ceremony.\",\n",
    "        \"‚ÄúThe strange case of Dr. Jekyll and Mr. Hyde‚Äù is a famous book... but I haven't read it.\",\n",
    "        \"I work for the C.I.A.. And you?\",\n",
    "        \"OMG #Twitter is sooooo coooool <3 :-) <-- lol...why do i write like this idk right? :) ü§∑üòÇ ü§ñ\"]\n",
    "\n",
    "for test_string in test_sentences:\n",
    "    print(my_tokenizer(test_string))\n",
    "    print(nltk_tokenizer(test_string))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUrQ_8EbNs2N"
   },
   "source": [
    "### Part 3 - Text classification with a unigram language model\n",
    "\n",
    "#### Training phase\n",
    "You now need to create the model and train it on the documents in the dataframe. Look at the scikit learn documentation to learn how to use the CountVectorizer and MultimodalNaiveBayes modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tQMy8K-MNs2N"
   },
   "outputs": [],
   "source": [
    "class CountVectorizer:\n",
    "   def __init__(self, regex = r\"\\b\\w+\\b\"):\n",
    "      self.vocabulary = defaultdict(int)\n",
    "      self.regex = re.compile(regex)\n",
    "      self.feature_names = []\n",
    "      \n",
    "   def tokenize(self, text):\n",
    "      text = self.lowercase(text)\n",
    "      return re.findall(self.regex, text)\n",
    "   \n",
    "   def lowercase(self, text):\n",
    "      return text.lower()\n",
    "   \n",
    "   def empty_vocab(self):\n",
    "      self.vocabulary = {}\n",
    "      \n",
    "   def fit(self, documents):\n",
    "      self.empty_vocab()\n",
    "      for doc in documents:\n",
    "         for word in set(self.tokenize(doc)):\n",
    "            if word not in self.vocabulary:\n",
    "               self.vocabulary[word] = len(self.vocabulary)\n",
    "               \n",
    "      self.feature_names = [None]  * len(self.vocabulary)\n",
    "      for word, index in self.vocabulary.items():\n",
    "         self.feature_names[index] =  word\n",
    "      return self\n",
    "   \n",
    "   def transform(self, documents):\n",
    "      X = np.zeros((len(documents), len(self.vocabulary)), dtype=np.int32)\n",
    "      for i, doc in enumerate(documents):\n",
    "         counts = Counter(self.tokenize(doc))\n",
    "         for word, count in counts.items():\n",
    "            index = self.vocabulary.get(word)\n",
    "            if index is not None:\n",
    "               X[i, index] = count\n",
    "               \n",
    "      return X\n",
    "  \n",
    "   def fit_transform(self, documents):\n",
    "      return self.fit(documents).transform(documents)\n",
    "   \n",
    "   def  get_feature_names(self):\n",
    "      return np.array(self.feature_names)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNaiveBayes:\n",
    "    def __init__(self, alpha=1.0, class_prior=None):\n",
    "        self.alpha = float(alpha)\n",
    "        self.class_prior = class_prior\n",
    "        self.classes_ = None\n",
    "        self.class_priors_ = class_prior  \n",
    "        self.likelihoods_ = None \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        if self.class_prior is not None:\n",
    "            priors = self.class_prior / self.class_prior.sum()\n",
    "        else:\n",
    "            counts = np.array([(y == c).sum() for c in self.classes_], dtype=float)\n",
    "            priors = counts / counts.sum()\n",
    "        self.class_priors_ = priors\n",
    "\n",
    "        self.likelihoods_ = self._calculate_likelihoods(X, y)\n",
    "        return self\n",
    "\n",
    "    def _calculate_likelihoods(self, X, y):\n",
    "        L = np.zeros((self.classes_.shape[0],  X.shape[1]), dtype=float)\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]                       \n",
    "            word_counts = np.asarray(X_c.sum(axis=0))\n",
    "            L[i, :] =(word_counts + self.alpha) / (word_counts.sum() + self.alpha * X.shape[1])\n",
    "        return L\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        log_prior = np.log(self.class_priors_)[None, :]              \n",
    "        log_lik_T = np.log(self.likelihoods_).T                      \n",
    "        log_scores = log_prior + X @ log_lik_T                     \n",
    "\n",
    "        log_scores -= log_scores.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(log_scores)\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR IMPLEMENTATION RESULTS\n",
      "Found  100 files\n",
      "Accuracy with laplace smoothing and no stopwords removal: 82.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marko Haraloviƒá\\AppData\\Local\\Temp\\ipykernel_30704\\2972429044.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lik_T = np.log(self.likelihoods_).T\n",
      "C:\\Users\\Marko Haraloviƒá\\AppData\\Local\\Temp\\ipykernel_30704\\2972429044.py:35: RuntimeWarning: invalid value encountered in matmul\n",
      "  log_scores = log_prior + X @ log_lik_T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without laplace smoothing and no stopwords removal: 50.00%\n",
      "Accuracy with laplace smoothing and stop words removed : 85.00%\n"
     ]
    }
   ],
   "source": [
    "# First, read all the test data from the files.  \n",
    "# Then classify it using the classifier you trained before\n",
    "# Finally, calculate the performance\n",
    "\n",
    "## Our implementation of vectorizer and bayesian network\n",
    "\n",
    "print(\"OUR IMPLEMENTATION RESULTS\")\n",
    "\n",
    "test_paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_data(test_paths)\n",
    "X_train = data['Review']\n",
    "y_train = data['Label']\n",
    "X_test = test_data['Review']\n",
    "y_test = test_data['Label']\n",
    "\n",
    "# w laplace\n",
    "model = MultiModalNaiveBayes(alpha=1)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "print(f\"Accuracy with laplace smoothing and no stopwords removal: {accuracy:.2f}%\")\n",
    "\n",
    "# without laplace\n",
    "model = MultiModalNaiveBayes(alpha=0.0)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "print(f\"Accuracy without laplace smoothing and no stopwords removal: {accuracy:.2f}%\")\n",
    "\n",
    "#Model with stop words removed:\n",
    "\n",
    "model = MultiModalNaiveBayes(alpha=1.0)\n",
    "\n",
    "vectorizer = CountVectorizerOff(stop_words=\"english\")\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "print(f\"Accuracy with laplace smoothing and stop words removed : {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFICIAL IMPLEMENTATION RESULTS\n",
      "Found  100 files\n",
      "Vocabulary size: 17989\n",
      "Accuracy with laplace smoothing and no stopwords removal : 84.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17989\n",
      "Accuracy  without laplace smoothing and no stopwords removal: 50.00%\n",
      "Vocabulary size: 17690\n",
      "Accuracy with laplace smoothing and with stopwords removal: 85.00%\n",
      "Vocabulary size: 20713\n",
      "Accuracy with laplace smoothing and no stopwords removal and without lowercasing: 81.00%\n"
     ]
    }
   ],
   "source": [
    "## Comparison witht he official implementations\n",
    "\n",
    "print(\"OFFICIAL IMPLEMENTATION RESULTS\")\n",
    "\n",
    "\n",
    "test_paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_data(test_paths)\n",
    "X_train = data['Review']\n",
    "y_train = data['Label']\n",
    "X_test = test_data['Review']\n",
    "y_test = test_data['Label']\n",
    "\n",
    "# with laplace smoothing and no stopwords removal\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "vectorizer = CountVectorizerOff()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy_basic = sum(predictions == y_test) / len(y_test)*100\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Accuracy with laplace smoothing and no stopwords removal : {accuracy_basic:.2f}%\")\n",
    "\n",
    "# without laplace smoothing and no stopwords removal\n",
    "model = MultinomialNB(alpha=0.0)\n",
    "vectorizer = CountVectorizerOff()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Accuracy  without laplace smoothing and no stopwords removal: {accuracy:.2f}%\")\n",
    "\n",
    "# with laplace smoothing and with stopwords removal\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "vectorizer = CountVectorizerOff(stop_words=\"english\")\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy_basic = sum(predictions == y_test) / len(y_test)*100\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Accuracy with laplace smoothing and with stopwords removal: {accuracy_basic:.2f}%\")\n",
    "\n",
    "# with laplace smoothing and without stopwords removal and without lowercase\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "vectorizer = CountVectorizerOff(lowercase=False)\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Accuracy with laplace smoothing and no stopwords removal and without lowercasing: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: The performance after disabling the default lowercase normalization (and\n",
    "without stop word removal). Is there a difference, and if so, why do you\n",
    "think there is one?\n",
    "A: The performance decreased slightly after disabling the default lowercase normalization. This is likely because the model can no longer treat words with different cases as the same word, leading to a larger vocabulary and sparser data, whcih can negatively impact the model's ability to learn (and generalize). It works a bit better (1pp) with stopwords removal, signaling that signal gotten from stopwords is marginally useful for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDCgEfYgNs2Q"
   },
   "source": [
    "### Part 4 - Text classification with a bigram language model\n",
    "\n",
    "Now we will classify the same dataset again, but this time with a bigram language model. \n",
    "\n",
    "#### Training phase\n",
    "Build a Na√Øve Bayes classifier that uses bigrams instead of single words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZSIam3ObNs2Q"
   },
   "outputs": [],
   "source": [
    "### Student code here ###\n",
    "\n",
    "class NgramModalNaiveBayes:\n",
    "   def __init__(self, alpha=1.0, class_prior=None):\n",
    "      self.alpha = float(alpha)\n",
    "      self.class_prior = class_prior\n",
    "      self.classes_ = None\n",
    "      self.likelihoods_ = None \n",
    "        \n",
    "   def fit(self, X, y):\n",
    "      y = np.asarray(y)\n",
    "      self.classes_ = np.unique(y)\n",
    "\n",
    "      if self.class_prior is not None:\n",
    "         priors = self.class_prior / self.class_prior.sum()\n",
    "      else:\n",
    "         counts = np.array([(y == c).sum() for c in self.classes_], dtype=float)\n",
    "         priors = counts / counts.sum()\n",
    "      self.class_priors_ = priors\n",
    "\n",
    "      self.likelihoods_ = self._calculate_likelihoods(X, y)\n",
    "      return self\n",
    "         \n",
    "   def _calculate_likelihoods(self, X, y):\n",
    "      L = np.zeros((self.classes_.shape[0],  X.shape[1]), dtype=float)\n",
    "      for i, c in enumerate(self.classes_):\n",
    "         X_c = X[y == c]                       \n",
    "         word_counts = np.asarray(X_c.sum(axis=0))\n",
    "         L[i, :] = (word_counts + self.alpha) / (word_counts.sum() + self.alpha * X.shape[1])\n",
    "      return L\n",
    "\n",
    "   def predict_proba(self, X):\n",
    "      log_prior = np.log(self.class_priors_)[None, :]              \n",
    "      log_lik_T = np.log(self.likelihoods_).T                      \n",
    "      log_scores = log_prior + X @ log_lik_T                     \n",
    "\n",
    "      log_scores -= log_scores.max(axis=1, keepdims=True)\n",
    "      probs = np.exp(log_scores)\n",
    "      probs /= probs.sum(axis=1, keepdims=True)\n",
    "      return probs\n",
    "\n",
    "   def predict(self, X):\n",
    "      return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vocabulary size: 136259\n",
      "Bigram training matrix shape: (1200, 136259)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Student code here ###\n",
    "vectorizer_bigram = CountVectorizerOff(ngram_range=(2, 2))\n",
    "\n",
    "X_train_bigram = vectorizer_bigram.fit_transform(X_train)\n",
    "print(f\"Bigram vocabulary size: {len(vectorizer_bigram.vocabulary_)}\")\n",
    "print(f\"Bigram training matrix shape: {X_train_bigram.shape}\")\n",
    "\n",
    "nb_bigram = MultinomialNB(alpha=1.0)\n",
    "nb_bigram.fit(X_train_bigram, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReyUDT1dNs2R"
   },
   "source": [
    "#### Testing phase\n",
    "As before, calculate the performance on your test data, and notice the difference with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6rkqDJENs2R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT IMPLEMETATION Accuracy of bigram : 89.00%\n",
      "OFFICIAL Bigram model accuracy: 89.0%\n",
      "\n",
      "Comparison:\n",
      "Unigram accuracy: 85.0%\n",
      "Bigram accuracy:  89.0%\n",
      "Difference: +4.0%\n"
     ]
    }
   ],
   "source": [
    "### Student code here ###\n",
    "\n",
    "# bigram, our implementation\n",
    "model = NgramModalNaiveBayes(alpha=1.0)\n",
    "vectorizer = CountVectorizerOff(ngram_range=(2, 2))\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "print(f\"OUT IMPLEMETATION Accuracy of bigram : {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "X_test_bigram = vectorizer_bigram.transform(test_data['Review'])\n",
    "\n",
    "y_pred_bigram = nb_bigram.predict(X_test_bigram)\n",
    "\n",
    "accuracy_bigram = accuracy_score(y_test, y_pred_bigram)\n",
    "print(f\"OFFICIAL Bigram model accuracy: {accuracy_bigram*100:}%\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Unigram accuracy: {accuracy_basic:}%\")\n",
    "print(f\"Bigram accuracy:  {accuracy_bigram*100:}%\")\n",
    "print(f\"Difference: {accuracy_bigram*100- accuracy_basic:+}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3K9vaB0Vzk"
   },
   "source": [
    "### Trigrams\n",
    "When I asked students how to improve the classification performance on this dataset, the first question was always \"use trigrams\" (or even higher-order n-grams). Let's try how much of an improvement that would be, by training a trigram model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U7htbTfeNs2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR IMPLEMENTATION Accuracy of trigram : 77.00%\n",
      "Trigram vocabulary size: 230924\n",
      "Bigram vocabulary size: 136259\n",
      "OFFICIAL Trigram model accuracy: 77.0%\n",
      "\n",
      "Results:\n",
      "Unigram accuracy:  85.0%\n",
      "Bigram accuracy:   89.0%\n",
      "Trigram accuracy:  77.0%\n"
     ]
    }
   ],
   "source": [
    "### Student code here ###\n",
    "\n",
    "# our implementation\n",
    "model = NgramModalNaiveBayes(alpha=1.0)\n",
    "vectorizer = CountVectorizerOff(ngram_range=(3,3))\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "predictions = model.predict(X_test_vectors)\n",
    "accuracy = sum(predictions == y_test) / len(y_test)*100\n",
    "print(f\"OUR IMPLEMENTATION Accuracy of trigram : {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "vectorizer_trigram = CountVectorizerOff(ngram_range=(3, 3))  # Only trigrams\n",
    "\n",
    "X_train_trigram = vectorizer_trigram.fit_transform(data['Review'])\n",
    "X_test_trigram = vectorizer_trigram.transform(test_data['Review'])\n",
    "\n",
    "print(f\"Trigram vocabulary size: {len(vectorizer_trigram.vocabulary_)}\")\n",
    "print(f\"Bigram vocabulary size: {len(vectorizer_bigram.vocabulary_)}\")\n",
    "\n",
    "nb_trigram = MultinomialNB(alpha=1.0)\n",
    "nb_trigram.fit(X_train_trigram, y_train)\n",
    "y_pred_trigram = nb_trigram.predict(X_test_trigram)\n",
    "accuracy_trigram = accuracy_score(y_test, y_pred_trigram)\n",
    "\n",
    "print(f\"OFFICIAL Trigram model accuracy: {accuracy_trigram*100:}%\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Unigram accuracy:  {accuracy_basic:}%\")\n",
    "print(f\"Bigram accuracy:   {accuracy_bigram*100:}%\")\n",
    "print(f\"Trigram accuracy:  {accuracy_trigram*100:}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT : Bigram outperforms unigram by 4pp, and trigram performs the worst. Trigram has a larger vocabulary size that the bigram and unigram, leading to a sparser training matrix. We think sparsity causes lower accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_2021_Homework2_FINAL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
