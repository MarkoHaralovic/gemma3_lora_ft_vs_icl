{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6e0166",
   "metadata": {},
   "source": [
    "# Natural Language Processing 2025-1A Homework 3\n",
    "\n",
    "# Vector Semantics, Word2Vec and LLMs \n",
    "\n",
    "Deadline: 1 October (23:59)\n",
    "\n",
    "Questions: Post them in the homework discussion on Canvas, sent them to nlp-course@utwente.nl or ask us during the practical sessions. \n",
    "\n",
    "How to submit: Please answer the questions directly in this notebook and submit it before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ad9bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Please Write your group number, your names with student IDs Here: \n",
    "\n",
    "##### Group: group 30\n",
    "##### Member names :\n",
    "- Marko Haralović, student id: 3758869\n",
    "- Sounic Akkaraju, student id: 3702111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705bb26",
   "metadata": {},
   "source": [
    "Make sure that the following libraries are up-to-date in your computation envrionment. It is highly recommended to work on this assignment in UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaaa5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in ./.local/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.local/lib/python3.10/site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in ./.local/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (1.13.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: spacy in ./.local/lib/python3.10/site-packages (3.8.7)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.local/lib/python3.10/site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.10/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.local/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.local/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in ./.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip3 install gensim nltk scikit-learn numpy pandas scipy \n",
    "!pip install  --upgrade gensim nltk scikit-learn numpy pandas scipy spacy ### Upgrade your libraries if neccesary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24ba63-c517-4ecf-bb9a-adce51f079b8",
   "metadata": {},
   "source": [
    "We'll need these libraries later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e976edb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import scipy, math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096be250-df10-4c24-85e5-86c5517d3c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13784d",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this assignment, you will first explore two types of word vectors: those generated using co-occurrence–based methods and those produced by the local-context predictive model Word2Vec. You will then apply and evaluate an NLP task powered by a Large Language Model (LLM). \n",
    "\n",
    "Note on Terminology: \n",
    "- The terms \"word\" and \"term\" are used interchangeably in this context, referring to unique tokens that you aim to represent as vectors. These tokens can be individual words, n-grams, phrases, or even identifiers, but for this assignment, we will focus on individual words.\n",
    "- Though \"word vectors\" and \"word embeddings\" are often used synonymously, they have distinct meanings.  According to [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding), conceptually, word embedding \"*involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension*\".\n",
    "\n",
    "# Part I. Co-occurrence count-based vectors\n",
    "\n",
    "Let's start with this corpus consisting of 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0db5b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents=[\n",
    "    \"The warm sun melts the icy snow on the mountain.\",\n",
    "    \"A warm cup of tea felt perfect in the cool morning air.\",\n",
    "    \"Her warm smile brightened the cold winter day.\",\n",
    "    \"I love the contrast of a warm blanket on a cold night.\",\n",
    "    \"The cold wind chilled me, but the warm fire offered comfort.\",\n",
    "    \"After a cold swim, the warm towel felt like heaven.\",\n",
    "    \"The warm colors of the sunset clashed with the cold breeze.\",\n",
    "    \"The chilly floor left her longing for the cozy comfort of slippers.\",\n",
    "    \"A gentle breeze eased the bite of the cold ocean waves.\",\n",
    "    \"Cold hands found solace in the warm pockets of his coat.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93419f",
   "metadata": {},
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")### Exercise 1.1.1 Construct the vocabulary (0.5 point)\n",
    "Before we construct co-occurrence matrices, we need to identify unique terms in the corpus, i.e. construct the vocabulary. You can remove stop words and apply other text normalisation operations before constructing the vocabulary. \n",
    "\n",
    "Tip: Sort your vocabulary alphabetically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4b28f2-1382-48c8-8f2a-712289c69ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11e7e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(vocabulary))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# your code ends here\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m vocab \u001b[38;5;241m=\u001b[39m construct_vocabulary(\u001b[43msents\u001b[49m, text_normalization \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlematization\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe size of the vocabulary is\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe words in the vocabulary are\u001b[39m\u001b[38;5;124m'\u001b[39m, vocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect unique terms in the corpus\n",
    "from typing import List, Optional\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# your code starts here\n",
    "def construct_vocabulary(sentences : List, text_normalization :str = \"lemmatization\", ignore_stopwords = True, lowercase = True):\n",
    "    vocabulary = set()\n",
    "    \n",
    "    regex = r'[a-zA-Z]+(?:\\.[a-zA-Z]+)*|\\d+|[.!?,:;()\"\\'](?=\\s)'\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\")) if ignore_stopwords else set()\n",
    "    \n",
    "    if text_normalization == \"lemmatization\":\n",
    "        normalization_fn = WordNetLemmatizer().lemmatize\n",
    "    elif text_normalization == \"stemming\":\n",
    "        normalization_fn = SnowballStemmer(\"english\", ignore_stopwords = ignore_stopwords).stem \n",
    "    else: \n",
    "        normalization_fn = None\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        if lowercase:\n",
    "            sentence = sentence.lower()\n",
    "        words = re.findall(regex, sentence)\n",
    "        if text_normalization == \"lemmatization\":\n",
    "            norm_words = [normalization_fn(w) for w in words if w.isalpha()]\n",
    "        elif text_normalization == \"stemming\":\n",
    "            norm_words = [normalization_fn(w) for w in words if w.isalpha()]\n",
    "        else:\n",
    "            norm_words = [w for w in words if w.isalpha()]\n",
    "        \n",
    "        if ignore_stopwords:\n",
    "            norm_words = [w for w in norm_words if w not in stop_words]\n",
    "        \n",
    "        vocabulary.update(norm_words)\n",
    "    return list(sorted(vocabulary))\n",
    "\n",
    "# your code ends here\n",
    "vocab = construct_vocabulary(sents, text_normalization = \"lematization\", ignore_stopwords = True, lowercase=True)\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The words in the vocabulary are', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9245a",
   "metadata": {},
   "source": [
    "### Co-Occurrence\n",
    "\n",
    "A co-occurrence matrix counts how often terms co-occur in certain context. The context can be a complete document, a sentence, or a sliding window. \n",
    "\n",
    "Tip: Check out the [sklearn.feature_extraction.text](https://scikit-learn.org/stable/api/sklearn.feature_extraction.html#module-sklearn.feature_extraction.text) submodule that gathers utilities to build feature vectors from text documents. \n",
    "\n",
    "### Exercise 1.1.2 Term-document occurrence matrix and term-term co-occurrence matrix (0.5 point)\n",
    "Let's first consider **each sentence** in the above corpus to be the context where the (co-)occurrences are counted. For example, the words *warm*, *sun*, *icy* and *snow* occur in the first sentence, therefore, they occur in this document and co-occur with each other. Going through all the sentences, you can construct the term-document occurrence matrix and term-term co-occurrence matrix. tdMatrixtdMatrixtdMatrixvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111b9b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the term-document matrix is (10, 51)\n",
      "The shape of thetdMatrix_pd matrix is (51, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>air</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bite</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blanket</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breeze</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brightened</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chilled</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chilly</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clashed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colors</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfort</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cozy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cup</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eased</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>felt</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gentle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hands</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heaven</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>icy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melts</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mountain</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocean</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offered</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pockets</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippers</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smile</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solace</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunset</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tea</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>towel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warm</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waves</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winter</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1   2   3   4   5   6   7   8   9   10\n",
       "air          0   1   0   0   0   0   0   0   0   0\n",
       "bite         0   0   0   0   0   0   0   0   1   0\n",
       "blanket      0   0   0   1   0   0   0   0   0   0\n",
       "breeze       0   0   0   0   0   0   1   0   1   0\n",
       "brightened   0   0   1   0   0   0   0   0   0   0\n",
       "chilled      0   0   0   0   1   0   0   0   0   0\n",
       "chilly       0   0   0   0   0   0   0   1   0   0\n",
       "clashed      0   0   0   0   0   0   1   0   0   0\n",
       "coat         0   0   0   0   0   0   0   0   0   1\n",
       "cold         0   0   1   1   1   1   1   0   1   1\n",
       "colors       0   0   0   0   0   0   1   0   0   0\n",
       "comfort      0   0   0   0   1   0   0   1   0   0\n",
       "contrast     0   0   0   1   0   0   0   0   0   0\n",
       "cool         0   1   0   0   0   0   0   0   0   0\n",
       "cozy         0   0   0   0   0   0   0   1   0   0\n",
       "cup          0   1   0   0   0   0   0   0   0   0\n",
       "day          0   0   1   0   0   0   0   0   0   0\n",
       "eased        0   0   0   0   0   0   0   0   1   0\n",
       "felt         0   1   0   0   0   1   0   0   0   0\n",
       "fire         0   0   0   0   1   0   0   0   0   0\n",
       "floor        0   0   0   0   0   0   0   1   0   0\n",
       "found        0   0   0   0   0   0   0   0   0   1\n",
       "gentle       0   0   0   0   0   0   0   0   1   0\n",
       "hands        0   0   0   0   0   0   0   0   0   1\n",
       "heaven       0   0   0   0   0   1   0   0   0   0\n",
       "icy          1   0   0   0   0   0   0   0   0   0\n",
       "left         0   0   0   0   0   0   0   1   0   0\n",
       "like         0   0   0   0   0   1   0   0   0   0\n",
       "longing      0   0   0   0   0   0   0   1   0   0\n",
       "love         0   0   0   1   0   0   0   0   0   0\n",
       "melts        1   0   0   0   0   0   0   0   0   0\n",
       "morning      0   1   0   0   0   0   0   0   0   0\n",
       "mountain     1   0   0   0   0   0   0   0   0   0\n",
       "night        0   0   0   1   0   0   0   0   0   0\n",
       "ocean        0   0   0   0   0   0   0   0   1   0\n",
       "offered      0   0   0   0   1   0   0   0   0   0\n",
       "perfect      0   1   0   0   0   0   0   0   0   0\n",
       "pockets      0   0   0   0   0   0   0   0   0   1\n",
       "slippers     0   0   0   0   0   0   0   1   0   0\n",
       "smile        0   0   1   0   0   0   0   0   0   0\n",
       "snow         1   0   0   0   0   0   0   0   0   0\n",
       "solace       0   0   0   0   0   0   0   0   0   1\n",
       "sun          1   0   0   0   0   0   0   0   0   0\n",
       "sunset       0   0   0   0   0   0   1   0   0   0\n",
       "swim         0   0   0   0   0   1   0   0   0   0\n",
       "tea          0   1   0   0   0   0   0   0   0   0\n",
       "towel        0   0   0   0   0   1   0   0   0   0\n",
       "warm         1   1   1   1   1   1   1   0   0   1\n",
       "waves        0   0   0   0   0   0   0   0   1   0\n",
       "wind         0   0   0   0   1   0   0   0   0   0\n",
       "winter       0   0   1   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the term-document occurrence matrix\n",
    "\n",
    "# your code starts here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "tdMatrix = vectorizer.fit_transform(sents)\n",
    "# your code ends here\n",
    "\n",
    "print('The shape of the term-document matrix is', tdMatrix.shape)\n",
    "tdMatrix_pd = pd.DataFrame.sparse.from_spmatrix(tdMatrix.T, index=vocab, columns=list(range(1, len(sents)+1)))\n",
    "print('The shape of thetdMatrix_pd matrix is', tdMatrix_pd.shape)\n",
    "tdMatrix_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980624f5",
   "metadata": {},
   "source": [
    "The term–term co-occurrence matrix can be computed directly from the term–document occurrence matrix. When doing so, pay close attention to the diagonal entries — they indicate self-co-occurrences, which may need to be removed or adjusted depending on your application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374d9fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the term-term matrix is (51, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air</th>\n",
       "      <th>bite</th>\n",
       "      <th>blanket</th>\n",
       "      <th>breeze</th>\n",
       "      <th>brightened</th>\n",
       "      <th>chilled</th>\n",
       "      <th>chilly</th>\n",
       "      <th>clashed</th>\n",
       "      <th>coat</th>\n",
       "      <th>cold</th>\n",
       "      <th>...</th>\n",
       "      <th>solace</th>\n",
       "      <th>sun</th>\n",
       "      <th>sunset</th>\n",
       "      <th>swim</th>\n",
       "      <th>tea</th>\n",
       "      <th>towel</th>\n",
       "      <th>warm</th>\n",
       "      <th>waves</th>\n",
       "      <th>wind</th>\n",
       "      <th>winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>air</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bite</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blanket</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breeze</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brightened</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chilled</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chilly</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clashed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colors</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfort</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cozy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cup</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eased</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>felt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gentle</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hands</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heaven</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>icy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melts</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mountain</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocean</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offered</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pockets</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slippers</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smile</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solace</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunset</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tea</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>towel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waves</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winter</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            air  bite  blanket  breeze  brightened  chilled  chilly  clashed  \\\n",
       "air           0     0        0       0           0        0       0        0   \n",
       "bite          0     0        0       1           0        0       0        0   \n",
       "blanket       0     0        0       0           0        0       0        0   \n",
       "breeze        0     1        0       0           0        0       0        1   \n",
       "brightened    0     0        0       0           0        0       0        0   \n",
       "chilled       0     0        0       0           0        0       0        0   \n",
       "chilly        0     0        0       0           0        0       0        0   \n",
       "clashed       0     0        0       1           0        0       0        0   \n",
       "coat          0     0        0       0           0        0       0        0   \n",
       "cold          0     1        1       2           1        1       0        1   \n",
       "colors        0     0        0       1           0        0       0        1   \n",
       "comfort       0     0        0       0           0        1       1        0   \n",
       "contrast      0     0        1       0           0        0       0        0   \n",
       "cool          1     0        0       0           0        0       0        0   \n",
       "cozy          0     0        0       0           0        0       1        0   \n",
       "cup           1     0        0       0           0        0       0        0   \n",
       "day           0     0        0       0           1        0       0        0   \n",
       "eased         0     1        0       1           0        0       0        0   \n",
       "felt          1     0        0       0           0        0       0        0   \n",
       "fire          0     0        0       0           0        1       0        0   \n",
       "floor         0     0        0       0           0        0       1        0   \n",
       "found         0     0        0       0           0        0       0        0   \n",
       "gentle        0     1        0       1           0        0       0        0   \n",
       "hands         0     0        0       0           0        0       0        0   \n",
       "heaven        0     0        0       0           0        0       0        0   \n",
       "icy           0     0        0       0           0        0       0        0   \n",
       "left          0     0        0       0           0        0       1        0   \n",
       "like          0     0        0       0           0        0       0        0   \n",
       "longing       0     0        0       0           0        0       1        0   \n",
       "love          0     0        1       0           0        0       0        0   \n",
       "melts         0     0        0       0           0        0       0        0   \n",
       "morning       1     0        0       0           0        0       0        0   \n",
       "mountain      0     0        0       0           0        0       0        0   \n",
       "night         0     0        1       0           0        0       0        0   \n",
       "ocean         0     1        0       1           0        0       0        0   \n",
       "offered       0     0        0       0           0        1       0        0   \n",
       "perfect       1     0        0       0           0        0       0        0   \n",
       "pockets       0     0        0       0           0        0       0        0   \n",
       "slippers      0     0        0       0           0        0       1        0   \n",
       "smile         0     0        0       0           1        0       0        0   \n",
       "snow          0     0        0       0           0        0       0        0   \n",
       "solace        0     0        0       0           0        0       0        0   \n",
       "sun           0     0        0       0           0        0       0        0   \n",
       "sunset        0     0        0       1           0        0       0        1   \n",
       "swim          0     0        0       0           0        0       0        0   \n",
       "tea           1     0        0       0           0        0       0        0   \n",
       "towel         0     0        0       0           0        0       0        0   \n",
       "warm          1     0        1       1           1        1       0        1   \n",
       "waves         0     1        0       1           0        0       0        0   \n",
       "wind          0     0        0       0           0        1       0        0   \n",
       "winter        0     0        0       0           1        0       0        0   \n",
       "\n",
       "            coat  cold  ...  solace  sun  sunset  swim  tea  towel  warm  \\\n",
       "air            0     0  ...       0    0       0     0    1      0     1   \n",
       "bite           0     1  ...       0    0       0     0    0      0     0   \n",
       "blanket        0     1  ...       0    0       0     0    0      0     1   \n",
       "breeze         0     2  ...       0    0       1     0    0      0     1   \n",
       "brightened     0     1  ...       0    0       0     0    0      0     1   \n",
       "chilled        0     1  ...       0    0       0     0    0      0     1   \n",
       "chilly         0     0  ...       0    0       0     0    0      0     0   \n",
       "clashed        0     1  ...       0    0       1     0    0      0     1   \n",
       "coat           0     1  ...       1    0       0     0    0      0     1   \n",
       "cold           1     0  ...       1    0       1     1    0      1     6   \n",
       "colors         0     1  ...       0    0       1     0    0      0     1   \n",
       "comfort        0     1  ...       0    0       0     0    0      0     1   \n",
       "contrast       0     1  ...       0    0       0     0    0      0     1   \n",
       "cool           0     0  ...       0    0       0     0    1      0     1   \n",
       "cozy           0     0  ...       0    0       0     0    0      0     0   \n",
       "cup            0     0  ...       0    0       0     0    1      0     1   \n",
       "day            0     1  ...       0    0       0     0    0      0     1   \n",
       "eased          0     1  ...       0    0       0     0    0      0     0   \n",
       "felt           0     1  ...       0    0       0     1    1      1     2   \n",
       "fire           0     1  ...       0    0       0     0    0      0     1   \n",
       "floor          0     0  ...       0    0       0     0    0      0     0   \n",
       "found          1     1  ...       1    0       0     0    0      0     1   \n",
       "gentle         0     1  ...       0    0       0     0    0      0     0   \n",
       "hands          1     1  ...       1    0       0     0    0      0     1   \n",
       "heaven         0     1  ...       0    0       0     1    0      1     1   \n",
       "icy            0     0  ...       0    1       0     0    0      0     1   \n",
       "left           0     0  ...       0    0       0     0    0      0     0   \n",
       "like           0     1  ...       0    0       0     1    0      1     1   \n",
       "longing        0     0  ...       0    0       0     0    0      0     0   \n",
       "love           0     1  ...       0    0       0     0    0      0     1   \n",
       "melts          0     0  ...       0    1       0     0    0      0     1   \n",
       "morning        0     0  ...       0    0       0     0    1      0     1   \n",
       "mountain       0     0  ...       0    1       0     0    0      0     1   \n",
       "night          0     1  ...       0    0       0     0    0      0     1   \n",
       "ocean          0     1  ...       0    0       0     0    0      0     0   \n",
       "offered        0     1  ...       0    0       0     0    0      0     1   \n",
       "perfect        0     0  ...       0    0       0     0    1      0     1   \n",
       "pockets        1     1  ...       1    0       0     0    0      0     1   \n",
       "slippers       0     0  ...       0    0       0     0    0      0     0   \n",
       "smile          0     1  ...       0    0       0     0    0      0     1   \n",
       "snow           0     0  ...       0    1       0     0    0      0     1   \n",
       "solace         1     1  ...       0    0       0     0    0      0     1   \n",
       "sun            0     0  ...       0    0       0     0    0      0     1   \n",
       "sunset         0     1  ...       0    0       0     0    0      0     1   \n",
       "swim           0     1  ...       0    0       0     0    0      1     1   \n",
       "tea            0     0  ...       0    0       0     0    0      0     1   \n",
       "towel          0     1  ...       0    0       0     1    0      0     1   \n",
       "warm           1     6  ...       1    1       1     1    1      1     0   \n",
       "waves          0     1  ...       0    0       0     0    0      0     0   \n",
       "wind           0     1  ...       0    0       0     0    0      0     1   \n",
       "winter         0     1  ...       0    0       0     0    0      0     1   \n",
       "\n",
       "            waves  wind  winter  \n",
       "air             0     0       0  \n",
       "bite            1     0       0  \n",
       "blanket         0     0       0  \n",
       "breeze          1     0       0  \n",
       "brightened      0     0       1  \n",
       "chilled         0     1       0  \n",
       "chilly          0     0       0  \n",
       "clashed         0     0       0  \n",
       "coat            0     0       0  \n",
       "cold            1     1       1  \n",
       "colors          0     0       0  \n",
       "comfort         0     1       0  \n",
       "contrast        0     0       0  \n",
       "cool            0     0       0  \n",
       "cozy            0     0       0  \n",
       "cup             0     0       0  \n",
       "day             0     0       1  \n",
       "eased           1     0       0  \n",
       "felt            0     0       0  \n",
       "fire            0     1       0  \n",
       "floor           0     0       0  \n",
       "found           0     0       0  \n",
       "gentle          1     0       0  \n",
       "hands           0     0       0  \n",
       "heaven          0     0       0  \n",
       "icy             0     0       0  \n",
       "left            0     0       0  \n",
       "like            0     0       0  \n",
       "longing         0     0       0  \n",
       "love            0     0       0  \n",
       "melts           0     0       0  \n",
       "morning         0     0       0  \n",
       "mountain        0     0       0  \n",
       "night           0     0       0  \n",
       "ocean           1     0       0  \n",
       "offered         0     1       0  \n",
       "perfect         0     0       0  \n",
       "pockets         0     0       0  \n",
       "slippers        0     0       0  \n",
       "smile           0     0       1  \n",
       "snow            0     0       0  \n",
       "solace          0     0       0  \n",
       "sun             0     0       0  \n",
       "sunset          0     0       0  \n",
       "swim            0     0       0  \n",
       "tea             0     0       0  \n",
       "towel           0     0       0  \n",
       "warm            0     1       1  \n",
       "waves           0     0       0  \n",
       "wind            0     0       0  \n",
       "winter          0     0       0  \n",
       "\n",
       "[51 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the term-term co-occurrence matrix\n",
    "# Be sure to handle the diagonal elements appropriately\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "ttMatrix = tdMatrix.T @ tdMatrix\n",
    "ttMatrix.setdiag(0)\n",
    "\n",
    "# your code ends here\n",
    "print('The shape of the term-term matrix is', ttMatrix.shape)\n",
    "ttMatrix_pd = pd.DataFrame.sparse.from_spmatrix(ttMatrix, index=vocab, columns=vocab)\n",
    "ttMatrix_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b760419c-f95d-4f97-9d47-cb37e2360708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold and warm co-occurr 6 times in total.\n"
     ]
    }
   ],
   "source": [
    "row, col = (ttMatrix_pd == ttMatrix_pd.to_numpy().max()).stack().idxmax()\n",
    "print(f\"{row} and {col} co-occurr {ttMatrix_pd.to_numpy().max()} times in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d773caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Based on term-term co-occurrence matrix, which pair(s) of words co-occur the most? \n",
    "\n",
    "**YOUR ANSWER**: \n",
    "Pais cold and warm co-occur the most out of all word pairs, with 6 co-occurrences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6b5a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1.2 Cosine similarity\n",
    "The benefit of vector semantics is that the similarity of two words can be computed as the cosine similarity between their vectors. Let's now compare how similar two words are. \n",
    "\n",
    "### Exercise 1.2.1 Calculate cosine similarity between words (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb97bc",
   "metadata": {},
   "source": [
    "What is the cosine similarity between \"cold\" and \"warm\" if 1) using term-document occurrence matrix 2) using term-term co-occurrence matrix?\n",
    "\n",
    "You may write your own cosine similarity function or use [`sklearn.metrics.pairwise.cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to calculate the pair-wise cosine similarity among all the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da533372-b700-4ed5-a8b4-44c070d69d51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of cold and word based on document-term co-oc matrix : 0.8017837257372731\n",
      "Similarity of cold and word based on term-term co-oc matrix : 0.3922143335085399\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between \"cold\" and \"ward\" using 1) using term-document occurrence matrix, \n",
    "# and 2) term-term co-occurrence matrix\n",
    "\n",
    "# your code starts here\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2)\n",
    "\n",
    "tdMatrixCosSim = cosine_similarity(tdMatrix_pd.loc[\"cold\"], tdMatrix_pd.loc[\"warm\"])\n",
    "ttMatrixCosSim = cosine_similarity(ttMatrix_pd.loc[\"cold\"], ttMatrix_pd.loc[\"warm\"])\n",
    "\n",
    "print(f\"Similarity of cold and word based on document-term co-oc matrix : {tdMatrixCosSim}\")\n",
    "print(f\"Similarity of cold and word based on term-term co-oc matrix : {ttMatrixCosSim}\")\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2ff1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1.2.2 (0.5 point)\n",
    "\n",
    "Now we can calculate cosine similarity between words using a co-occurrence matrix. You can choose any previously constructed matrix for the similarity calculation. Rank all the words based on their similarity to the word *cold*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec803dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using term-document matrix:\n",
      "warm : 0.8017837257372731\n",
      "breeze : 0.5345224838248487\n",
      "bite : 0.3779644730092272\n",
      "blanket : 0.3779644730092272\n",
      "brightened : 0.3779644730092272\n",
      "chilled : 0.3779644730092272\n",
      "clashed : 0.3779644730092272\n",
      "coat : 0.3779644730092272\n",
      "colors : 0.3779644730092272\n",
      "contrast : 0.3779644730092272\n",
      "day : 0.3779644730092272\n",
      "eased : 0.3779644730092272\n",
      "fire : 0.3779644730092272\n",
      "found : 0.3779644730092272\n",
      "gentle : 0.3779644730092272\n",
      "hands : 0.3779644730092272\n",
      "heaven : 0.3779644730092272\n",
      "like : 0.3779644730092272\n",
      "love : 0.3779644730092272\n",
      "night : 0.3779644730092272\n",
      "ocean : 0.3779644730092272\n",
      "offered : 0.3779644730092272\n",
      "pockets : 0.3779644730092272\n",
      "smile : 0.3779644730092272\n",
      "solace : 0.3779644730092272\n",
      "sunset : 0.3779644730092272\n",
      "swim : 0.3779644730092272\n",
      "towel : 0.3779644730092272\n",
      "waves : 0.3779644730092272\n",
      "wind : 0.3779644730092272\n",
      "winter : 0.3779644730092272\n",
      "comfort : 0.26726124191242434\n",
      "felt : 0.26726124191242434\n",
      "air : 0.0\n",
      "chilly : 0.0\n",
      "cool : 0.0\n",
      "cozy : 0.0\n",
      "cup : 0.0\n",
      "floor : 0.0\n",
      "icy : 0.0\n",
      "left : 0.0\n",
      "longing : 0.0\n",
      "melts : 0.0\n",
      "morning : 0.0\n",
      "mountain : 0.0\n",
      "perfect : 0.0\n",
      "slippers : 0.0\n",
      "snow : 0.0\n",
      "sun : 0.0\n",
      "tea : 0.0\n",
      "\n",
      "Using term-term matrix:\n",
      "clashed : 0.5307448924342753\n",
      "colors : 0.5307448924342753\n",
      "sunset : 0.5307448924342753\n",
      "felt : 0.49028113042871646\n",
      "chilled : 0.4845015831115092\n",
      "coat : 0.4845015831115092\n",
      "fire : 0.4845015831115092\n",
      "found : 0.4845015831115092\n",
      "hands : 0.4845015831115092\n",
      "heaven : 0.4845015831115092\n",
      "like : 0.4845015831115092\n",
      "offered : 0.4845015831115092\n",
      "pockets : 0.4845015831115092\n",
      "solace : 0.4845015831115092\n",
      "swim : 0.4845015831115092\n",
      "towel : 0.4845015831115092\n",
      "wind : 0.4845015831115092\n",
      "blanket : 0.4776704031908477\n",
      "brightened : 0.4776704031908477\n",
      "contrast : 0.4776704031908477\n",
      "day : 0.4776704031908477\n",
      "love : 0.4776704031908477\n",
      "night : 0.4776704031908477\n",
      "smile : 0.4776704031908477\n",
      "winter : 0.4776704031908477\n",
      "breeze : 0.4608156130736432\n",
      "warm : 0.3922143335085399\n",
      "comfort : 0.34259435491376583\n",
      "icy : 0.31844693546056513\n",
      "melts : 0.31844693546056513\n",
      "mountain : 0.31844693546056513\n",
      "snow : 0.31844693546056513\n",
      "sun : 0.31844693546056513\n",
      "air : 0.3139929128113796\n",
      "cool : 0.3139929128113796\n",
      "cup : 0.3139929128113796\n",
      "morning : 0.3139929128113796\n",
      "perfect : 0.3139929128113796\n",
      "tea : 0.3139929128113796\n",
      "bite : 0.29070094986690553\n",
      "eased : 0.29070094986690553\n",
      "gentle : 0.29070094986690553\n",
      "ocean : 0.29070094986690553\n",
      "waves : 0.29070094986690553\n",
      "chilly : 0.04845015831115092\n",
      "cozy : 0.04845015831115092\n",
      "floor : 0.04845015831115092\n",
      "left : 0.04845015831115092\n",
      "longing : 0.04845015831115092\n",
      "slippers : 0.04845015831115092\n"
     ]
    }
   ],
   "source": [
    "# Rank all the words by their similarity to word \"cold\"\n",
    "\n",
    "# your code starts here\n",
    "distances = []\n",
    "for word in vocab:\n",
    "    if word != \"cold\":\n",
    "        sim = cosine_similarity(tdMatrix_pd.loc[\"cold\"], tdMatrix_pd.loc[word])\n",
    "        distances.append((sim, word))\n",
    "\n",
    "distances = sorted(distances, key=lambda x: x[0], reverse=True)\n",
    "print(\"Using term-document matrix:\")\n",
    "for sim, word in distances:\n",
    "    print(word, \":\", sim)\n",
    "\n",
    "distances = []\n",
    "for word in vocab:\n",
    "    if word != \"cold\":\n",
    "        sim = cosine_similarity(ttMatrix_pd.loc[\"cold\"], ttMatrix_pd.loc[word])\n",
    "        distances.append((sim, word))\n",
    "\n",
    "distances = sorted(distances, key=lambda x: x[0], reverse=True)\n",
    "print(\"\\nUsing term-term matrix:\")\n",
    "for sim, word in distances:\n",
    "    print(word, \":\", sim)\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cc631",
   "metadata": {
    "tags": []
   },
   "source": [
    "The calculated cosine similarity does not appear to capture semantic similarity or relatedness reliably. How might we obtain more meaningful similarity measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c1612",
   "metadata": {
    "tags": []
   },
   "source": [
    "**YOUR ANSWER**: In the lecture we introduced TF-IDF, we think that would help with the results, so lower number of all too common words wouldnt be returned. We are also looking at the term-document occurences, an upgrade would be to have a sliding window approach to determine if the words appear in the same context and in the same position in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221fd24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1.3 TF-IDF\n",
    "\n",
    "## Excercise 1.3.1 (0.5 point)\n",
    "For the above corpus, construct a TF-IDF weighted term-document matrix, using [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9342915-d753-4c46-a79c-3c12d7bc02b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the TF-IDF matrix is (10, 62)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.398599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air</th>\n",
       "      <td>0</td>\n",
       "      <td>0.352077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bite</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blanket</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breeze</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warm</th>\n",
       "      <td>0.166014</td>\n",
       "      <td>0.156291</td>\n",
       "      <td>0.192932</td>\n",
       "      <td>0.187886</td>\n",
       "      <td>0.159932</td>\n",
       "      <td>0.176943</td>\n",
       "      <td>0.171809</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.161213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waves</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winter</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "after           0         0         0         0         0  0.398599         0   \n",
       "air             0  0.352077         0         0         0         0         0   \n",
       "bite            0         0         0         0         0         0         0   \n",
       "blanket         0         0         0   0.42325         0         0         0   \n",
       "breeze          0         0         0         0         0         0  0.329015   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "warm     0.166014  0.156291  0.192932  0.187886  0.159932  0.176943  0.171809   \n",
       "waves           0         0         0         0         0         0         0   \n",
       "wind            0         0         0         0  0.360279         0         0   \n",
       "winter          0         0  0.434617         0         0         0         0   \n",
       "with            0         0         0         0         0         0  0.387035   \n",
       "\n",
       "         7         8         9  \n",
       "after    0         0         0  \n",
       "air      0         0         0  \n",
       "bite     0  0.383617         0  \n",
       "blanket  0         0         0  \n",
       "breeze   0   0.32611         0  \n",
       "...     ..       ...       ...  \n",
       "warm     0         0  0.161213  \n",
       "waves    0  0.383617         0  \n",
       "wind     0         0         0  \n",
       "winter   0         0         0  \n",
       "with     0         0         0  \n",
       "\n",
       "[62 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a TF-IDF weighted term-document matrix\n",
    "\n",
    "# your code starts here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfIdfMatrix = vectorizer.fit_transform(sents)\n",
    "print('The shape of the TF-IDF matrix is', tfIdfMatrix.shape)\n",
    "tfIdfMatrix_pd = pd.DataFrame.sparse.from_spmatrix(tfIdfMatrix.T, index=vectorizer.get_feature_names_out(), columns=list(range(len(sents))))\n",
    "tfIdfMatrix_pd\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440cf7",
   "metadata": {},
   "source": [
    "Compute and rank the words in descending order based on their similarity to *cold*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57eb8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF matrix:\n",
      "warm : 0.821426502771318\n",
      "breeze : 0.5230226303522726\n",
      "brightened : 0.41713832678268736\n",
      "day : 0.41713832678268736\n",
      "smile : 0.41713832678268736\n",
      "winter : 0.41713832678268736\n",
      "blanket : 0.4062281511043087\n",
      "contrast : 0.4062281511043087\n",
      "love : 0.4062281511043087\n",
      "night : 0.4062281511043087\n",
      "heaven : 0.3825681770642013\n",
      "like : 0.3825681770642013\n",
      "swim : 0.3825681770642013\n",
      "towel : 0.3825681770642013\n",
      "clashed : 0.3714692891396049\n",
      "colors : 0.3714692891396049\n",
      "sunset : 0.3714692891396049\n",
      "bite : 0.3681891349384532\n",
      "eased : 0.3681891349384532\n",
      "gentle : 0.3681891349384532\n",
      "ocean : 0.3681891349384532\n",
      "waves : 0.3681891349384532\n",
      "coat : 0.34855809088144746\n",
      "found : 0.34855809088144746\n",
      "hands : 0.34855809088144746\n",
      "pockets : 0.34855809088144746\n",
      "solace : 0.34855809088144746\n",
      "chilled : 0.3457896483377711\n",
      "fire : 0.3457896483377711\n",
      "offered : 0.3457896483377711\n",
      "wind : 0.3457896483377711\n",
      "felt : 0.2867312920148833\n",
      "comfort : 0.2556039270346906\n",
      "air : 0.0\n",
      "chilly : 0.0\n",
      "cool : 0.0\n",
      "cozy : 0.0\n",
      "cup : 0.0\n",
      "floor : 0.0\n",
      "icy : 0.0\n",
      "left : 0.0\n",
      "longing : 0.0\n",
      "melts : 0.0\n",
      "morning : 0.0\n",
      "mountain : 0.0\n",
      "perfect : 0.0\n",
      "slippers : 0.0\n",
      "snow : 0.0\n",
      "sun : 0.0\n",
      "tea : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute and rank the words in descending order based on their similarity to *cold*\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "distances = []\n",
    "for word in vocab:\n",
    "    if word != \"cold\":\n",
    "        sim = cosine_similarity(tfIdfMatrix_pd.loc[\"cold\"], tfIdfMatrix_pd.loc[word])\n",
    "        distances.append((sim, word))\n",
    "\n",
    "distances = sorted(distances, key=lambda x: x[0], reverse=True)\n",
    "print(\"Using TF-IDF matrix:\")\n",
    "for sim, word in distances:\n",
    "    print(word, \":\", sim)\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60efccf",
   "metadata": {},
   "source": [
    "## Exercise 1.3.2 (0.5 point)\n",
    "\n",
    "Let's use a bigger dataset which contains 2225 BBC news articles to construct TF-IDF term-document matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b725e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the TF-IDF matrix is (2226, 29421)\n",
      "The size of the vocabulary is 24713\n",
      "The shape of the term-document matrix is (29421, 2226)\n"
     ]
    }
   ],
   "source": [
    "sents=codecs.open('bbc-text.csv','r', encoding='utf-8').readlines() # load the data\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "vocab = construct_vocabulary(sents, text_normalization = \"lemmatization\", ignore_stopwords = True, lowercase=True)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfIdfMatrix = vectorizer.fit_transform(sents)\n",
    "print('The shape of the TF-IDF matrix is', tfIdfMatrix.shape)\n",
    "term_doc_matrix = pd.DataFrame.sparse.from_spmatrix(tfIdfMatrix.T, index=vectorizer.get_feature_names_out(), columns=list(range(len(sents))))\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The shape of the term-document matrix is', term_doc_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31793fb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can compute which words are most similar to *cold*. Does this list of words make more sense now and why?\n",
    "\n",
    "**YOUR ANSWER**: The words are not really similar to cold. For instance, heating makes sense to be connected to cold, but it is not similar per say, it's the opposite.Some of other words are good examples, but most of them are completely not useful, like \"opec\", \"adnan\", \"eldin\", \"wolfram\", \"teag\", \"brent\", etc. We were surprised by these results as we'd deem them as not that useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "644c0120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'cold':\n",
      "opec : 0.485556\n",
      "heating : 0.438346\n",
      "crude : 0.437255\n",
      "brent : 0.404701\n",
      "cartel : 0.396567\n",
      "barrel : 0.389455\n",
      "retreated : 0.385683\n",
      "adnan : 0.385683\n",
      "eldin : 0.385683\n",
      "conservation : 0.384195\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 words that are most similar to word \"cold\"\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "def find_similar_words(matrix, target_term, top_n):\n",
    "    vec_target = matrix.loc[target_term].sparse.to_dense().values.reshape(1, -1)\n",
    "    all_vecs = matrix.sparse.to_dense().values\n",
    "    sims = cosine_similarity(vec_target, all_vecs).ravel()\n",
    "\n",
    "    sims[matrix.index.get_loc(target_term)] = -np.inf\n",
    "\n",
    "    top_idx = np.argsort(-sims)[:top_n]\n",
    "    return top_idx, sims\n",
    "\n",
    "top_idx, sims = find_similar_words(term_doc_matrix, \"cold\", 10)\n",
    "\n",
    "print(f\"Top {len(top_idx)} words similar to 'cold':\")\n",
    "for i in top_idx:\n",
    "    print(f\"{term_doc_matrix.index[i]} : {sims[i]:.6f}\")\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6e5f12-d384-47ae-bb74-9c3e055f9da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max TF-IDF: term 'commodore' in document 1204 with weight 0.747\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "M = tfIdfMatrix.T.tocoo()  # (n_terms, n_docs)\n",
    "i = M.data.argmax()\n",
    "term = terms[M.row[i]]\n",
    "doc_id = M.col[i]\n",
    "val = M.data[i]\n",
    "print(f\"Max TF-IDF: term '{term}' in document {doc_id} with weight {val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206eaef",
   "metadata": {},
   "source": [
    "Find another 3 pairs of words whose cosine similarity makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51110f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for 3 pairs of words whose cosine similarities reflect their semantic similarity or relatedness.\n",
    "\n",
    "def pair_similarity(vectorizer, tfidf, w1, w2):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    m = {t:i for i,t in enumerate(terms)}\n",
    "    if w1 not in m or w2 not in m:\n",
    "        return None\n",
    "    return float(cosine_similarity(tfidf[m[w1]], tfidf[m[w2]])[0,0])\n",
    "\n",
    "pairs = [\n",
    "    (\"oil\",\"crude\"),\n",
    "    (\"shares\",\"stocks\"),\n",
    "    (\"government\",\"minister\"),\n",
    "    (\"football\",\"match\"),\n",
    "    (\"winter\",\"snow\"),\n",
    "    (\"economy\",\"growth\"),\n",
    "    (\"court\",\"judge\"),\n",
    "    (\"ball\", \"sport\"),\n",
    "    (\"economy\",\"state\"),\n",
    "    (\"court\",\"lawyer\"),\n",
    "    (\"car\",\"driver\"),\n",
    "    (\"light\",\"electricity\"),\n",
    "    (\"rain\",\"weather\"),\n",
    "    (\"doctor\",\"hospital\"),\n",
    "    (\"market\",\"trade\")\n",
    "]\n",
    "\n",
    "scored = [(p, pair_similarity(vectorizer, tfIdfMatrix.T, *p)) for p in pairs]\n",
    "scored = [(p,s) for p,s in scored if s is not None]\n",
    "scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for (w1,w2), s in scored[:3]:\n",
    "    print(f\"{w1} – {w2}: {s:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf1757",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part II. Word2Vec word vectors\n",
    "\n",
    "Here, we explore the embeddings produced by word2vec. Please read J&M 6.8 or the [original paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) if you are interested in the details of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbd4ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2.1 Pre-train word2vec model\n",
    "\n",
    "Run the following script to load the word2vec vectors into memory. **Note**: This might take several minutes. If you run out of memory, try closing other applicaions or restart your machine to free more memory. \n",
    "\n",
    "Please note, the following experiments run with Gensim 4.3.3. If you are still running an old version of Gensim, please upgrade your Gensim library or check [Migrating from Gensim 3.x to 4](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to adapt your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 million Word2Vec Vectors, pre-trained on Google news, each with the dimension of 300\n",
    "# This model may take a few minutes to load.\n",
    "\n",
    "import gensim.downloader as api\n",
    "start_time = time.time()\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f880003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded vocab size {}\".format(len(w2v_google.index_to_key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd53f8",
   "metadata": {},
   "source": [
    "Once the model is loaded, you can extract the vector for individual words directly using `wv_google['']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c53603-63fe-40f7-b510-0423530c7ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_google['cold'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e956e1",
   "metadata": {},
   "source": [
    "One of the property of semantic embedding is that similar words are embedded close to each other. Use  `w2v_google.most_similar()` to identify the most similar words to *north*. Does this list make more sense to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for w,c in w2v_google.most_similar('cold'):\n",
    "    print(w,c)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e545c5",
   "metadata": {},
   "source": [
    "Check a few more words to see whether their most similar words make sense to you and explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3a82b-8a43-427c-aa60-e64f683b8ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_google.most_similar('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c41118-16e4-409c-b466-25479c3925ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_google.most_similar('cheese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f427cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word analogies\n",
    "\n",
    "An analogy explains one thing in terms of another to highlight the ways in which they are alike. For example, *paris* is similar to *france* in the same way that *rome* is to *italy*. Word2Vec vectors sometimes shows the ability of solving analogy problem of the form **a is to b as a* is to what?**.\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value). In the case below, the top one word *italy* is the answer, so this analogy is solved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- paris : france :: rome : x\n",
    "print(w2v_google.most_similar(positive=['rome', 'france'], negative=['paris']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c654a",
   "metadata": {},
   "source": [
    "### Exercise 2.1.1 (0.5 point)\n",
    "Look for one analogy that can be solved successfully and one analogy that could not be solved using this pre-trained Word2Vec model. Check out [this paper](https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0) for inspirations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your successful case goes here\n",
    "print(w2v_google.most_similar(positive=['good', 'bigger'], negative=['big']))\n",
    "\n",
    "# Your failed case goes here\n",
    "print(w2v_google.most_similar(positive=['devil', 'god'], negative=['paradise'])) # were expecting devil\n",
    "\n",
    "# this is also a possible failcase in case bias is classified as a negative\n",
    "print(w2v_google.most_similar(positive=['nurse', 'man'], negative=['plumber']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fe0ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualising word analogies\n",
    "\n",
    "The following cell shows you how to use [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualise a set of words based on their embeddings. You can also apply other dimensionality reduction methods (e.g. [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)) to reduce the vectors from 300-dimensional to 2 dimensional. \n",
    "\n",
    "Please note, reducing dimensionality from 300 to 2 is a very challenging task. You can try different parameters in the tSNE and see their effects on the final visualisation. In particular, the visualisation is very sensitive to the perplexity value that you give. Please try a few different perplexity valuse and keep the one that gives the most reasonable visusalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tsne_plot(model, wordlist, p):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=p, n_components=2, init='pca', max_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(9,9))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "wordlist = ['man', 'woman', 'nephew', 'niece', 'brother', 'sister', 'uncle', 'aunt']\n",
    "tsne_plot(w2v_google, wordlist, len(wordlist)-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcd4c5",
   "metadata": {},
   "source": [
    "### Exercise 2.1.2 (0.5 point)\n",
    "Find another group analogies (at least 3 pairs of words) and see how they are visualised.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c486225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare at least 3 pairs of words   \n",
    "\n",
    "# your answer goes here\n",
    "wordlist = ['italy','rome','france','paris','spain','madrid','germany','berlin']\n",
    "\n",
    "p=len(wordlist)-1\n",
    "tsne_plot(w2v_google,wordlist,p-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa0b36",
   "metadata": {},
   "source": [
    "### Exercise 2.1.3  Synonyms and antonyms (0.5 point)\n",
    "\n",
    "\n",
    "\n",
    "Find three words (w1, w2, w3) so that \n",
    "- w1 and w2 are synonyms, \n",
    "- w1 and w3 are antonyms, \n",
    "- cosine_distance(w1, w2) > cosine_distance(w1, w3) or cosine_distance(w1, w2) $\\approx$ cosine_distance(w1, w3). \n",
    "\n",
    "Please give a possible explanation for why this has happened. \n",
    "\n",
    "You can use [`w2v_google.distance()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the cosine distance between two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b68367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace XXX, YYY and ZZZ with your chosen words\n",
    "\n",
    "w1='sweet'\n",
    "w2a='sweetened'\n",
    "w2b='sugary'\n",
    "w3='salty'\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2a, w2v_google.distance(w1, w2a)))\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2b, w2v_google.distance(w1, w2b)))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w2v_google.distance(w1, w3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee08b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer**:  Sweet is closer to salty than to sweetened, and similarly as close to salty as to sugary. Our explanation would be that, in this case, sweet and salty occur more frequently in the same context in sentences together (we thought of recipes, food lists, reviews, in which such words could both occur). While sweetened is a synonym, we argue that sweetened is not that often used in the same context as sweet, while salty is. The fact that salty is an antonym is overridden by the fact that it appears more frequently in the sliding window around sweet. Sugary is an example of a synonym that is often used to describe food, and is obviously present more often in the context window around sweet; therefore, the distance between sweet and sugary matches the difference between sweet and salty.\n",
    "NOTE: sweet is also a polysemous word, but in this case we refer to the literate meaning of the sweet, characterizing taste of something.w1='equal'\n",
    "w2a='equivalent'\n",
    "w2b='identical'\n",
    "w3='unequal'\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2a, w2v_google.distance(w1, w2a)))\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2b, w2v_google.distance(w1, w2b)))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w2v_google.distance(w1, w3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f3496-70e2-4dbf-9c7c-f272f7b5c8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1='equal'\n",
    "w2a='equivalent'\n",
    "w2b='identical'\n",
    "w3='unequal'\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2a, w2v_google.distance(w1, w2a)))\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2b, w2v_google.distance(w1, w2b)))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w2v_google.distance(w1, w3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78159",
   "metadata": {},
   "source": [
    "### Exercise 2.1.4 Polysemous Words (0.5 point)\n",
    "\n",
    "Some words are polysemous, i.e. they have multiple meanings. For example the word *bank* can be a financial institute or the rising ground bordering a lake or river. Find a polysemous word whose top most similar words contains related words from multiple meanings. You should use the the [`wv_google.most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the closet neighbours of the word. You may increase the number of neighbours in order to identify multiple groups of meanings. Submit the ranked word list and explained how the words are grouped into different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('crane',topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "focusword='crane'\n",
    "wordlist=[focusword]\n",
    "for w in w2v_google.most_similar(focusword,topn=100):\n",
    "    wordlist.append(w[0])\n",
    "print(wordlist)\n",
    "tsne_plot(w2v_google,wordlist,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a54cf-0133-422d-8488-71f6c3e698cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## OR EXAMPLE\n",
    "w2v_google.most_similar('nice',topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6dda9b-0be4-4567-a28b-2a1a47e31feb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Identified groups of words similar to nice grouped by meaning:\n",
    "- adjective/a quality of a person,: good, great, wonderful, terrific, fantastic, awesome, amazing, fabulous, marvelous, perfect, lovely, classy, decent, nifty\n",
    "- beauty and looks : beautiful, gorgeous, pretty\n",
    "- word related to nice as being a approvl of smth: neat, fantastic, definetely, perfect, interesting, okay\n",
    "- degree words: really, definitely, pretty, nicely, maybe, kinda, alright, okay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efed6b",
   "metadata": {},
   "source": [
    "Look into literature and describe potential methods to address this polysymy issue in word embeddings. Please cite the papers that you refer to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbab96",
   "metadata": {
    "tags": []
   },
   "source": [
    "**YOUR ANSWER**: [1] suggest training a multiple embeddings for each of the words. Idea is to perform a forward pass through the model and extrapolate different embeddings from a singular vector representation, hypothesizing that that would lead to performance gain, since we are increasing expressivity, since each embedding that would represent a word could be used by the model (learned to do so) to properly represent the word. One vector could represent base meaning of the word, others could pick up other meaning and contexts in which the word appears. [2] proposes a two-stage method. First, learn global embeddings, then in the second stage adaptievely learn the local embeddings in the latent space to model polysemy. [3]  propose an LSTM with attentional multi-sense embeddings that assigns multiple sense vectors to each word.\n",
    "\n",
    "ARTICLES\n",
    "- [1] https://dl.acm.org/doi/abs/10.1145/3206098.3206101 \n",
    "- [2] https://www.sciencedirect.com/science/article/pii/S0950705121000903\n",
    "- [3] https://ieeexplore.ieee.org/document/9053503"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18347b85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2.2  Self-trained Word2Vec model\n",
    "\n",
    "The word2vec model that we have been using so far is pre-trained on Google news. This is suitable for applications involving general topics. However, for special domains, such as scientific or medical domain, some domain-specific semantics could not be captured in the pre-trained model. Fortunately, word2vec is pretty efficient in training from scratch. We will use two different datasets to observer the effect on the input corpus. \n",
    "\n",
    "Importance parameters are highlighted in bold. Please choose a few different values and see their effects.  \n",
    "\n",
    "- class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, **vector_size=100**, alpha=0.025, **window=5**, **min_count=5**, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, **sg=0**, hs=0, **negative=5**, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
    "    \n",
    "Please check the [gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for more assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most similar words to 'young' in Google news\n",
    "w2v_google.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bc89c",
   "metadata": {},
   "source": [
    "### Exercise 2.2.1 (1 point)\n",
    "\n",
    "We first train a word2vec model on the corpus consisting the abstracts from 111K astrophysics/astronomy articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f9745-9e97-4621-bf36-00b84356cf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6497b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take up a few minutes to train.\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('astro_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "# sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "w2v_astro = Word2Vec(sentences = sentences, vector_size = 300, window = 5, min_count = 5, sg = 0, negative = 5, workers=os.cpu_count())\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc8a04",
   "metadata": {},
   "source": [
    "If all goes well, you may see *pms*, *proto* or *yso* among the top 10 most similar words to *young*. If you are curious, protostars and pre-main-sequence (PMS) stars are all [Young Stella Objects](https://en.wikipedia.org/wiki/Young_stellar_object)  (YSOs). Here, “young” means pre-main-sequence. For low-mass stars, this means ages of $10^5$ to $10^8$ years. [Ref](https://nexsci.caltech.edu/workshop/2003/2003_MSS/10_Thursday/mss2003_jensen.pdf)\n",
    "\n",
    "We then train a word2vec model on the corpus consisting of nearly 479K [Medline](https://www.nlm.nih.gov/medline/medline_overview.html) articles. Note, this corpus is rather big. If this is too much for your local machine, use UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab) or [Google Colab](https://colab.research.google.com/notebooks/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5988b3-a83c-4a12-9b1f-eb4060bed778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import os\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch+1} start\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f\"Epoch {self.epoch+1} end. Loss: {loss}\")\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take up to half an hour to train!\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('medline_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "w2v_medline = Word2Vec(sentences = sentences, vector_size = 300, window = 5, min_count = 5, sg = 0, negative = 5, workers=os.cpu_count(),\n",
    "                      compute_loss=True, callbacks=[EpochLogger()])\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40661b",
   "metadata": {},
   "source": [
    "Find another word and compute its most similar words based on different models. Please explain why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb95a96",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('human')  # Replace XXX with your chosen word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892977b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba7781",
   "metadata": {
    "tags": []
   },
   "source": [
    "**YOUR ANSWER to Why this happens:** Word2Vec finds similar words based on the corpus at hand. Given that corpora differ significantly, variations are expected. It is clear that the word human will not be used in the same context across domains; therefore, it makes sense that similar words to the target word differ across domains, as the algorithms are highly dependent on the data. In case of astro dataset, we'd expect astronaut to be similar, and it was. In case of medline dataset, we expected  more medical terms, which were indeed returned (eg. extraembryonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c763b",
   "metadata": {},
   "source": [
    "### Exercise 2.2.2 (1 point)\n",
    "\n",
    "Experiment with different parameters, for example, the vector size, the window size, the minimal count, skip-gram or CBOW, etc. Observe their effects on the quality of the word embeddings and/or computational cost.\n",
    "\n",
    "You can apply intrinsic evaluations to compare the quality of your models. For example, your can check the correlation with human opinion on word similarity or on word analogies. Check gensim documentations for more options. For example, [evaluate_word_analogies](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies) and [evaluate_word_pairs](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352924ca",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b116b6c-785f-4fe8-aabf-9a4824a3a3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogies_path = \"./questions-words.txt\"\n",
    "\n",
    "\n",
    "overall_accuracy, sections = w2v_google.evaluate_word_analogies(\n",
    "    analogies_path,\n",
    "    case_insensitive=True \n",
    ")\n",
    "\n",
    "print(f\"Totall acc for w2v_google : {overall_accuracy:.4f}\")\n",
    "\n",
    "for sec in sections:\n",
    "    n_correct, n_incorrect = len(sec['correct']), len(sec['incorrect'])\n",
    "    acc = n_correct / (n_correct + n_incorrect)\n",
    "    print(f\"{sec['section']:<35} | n={n_correct + n_incorrect:<4} | acc={acc:.4f}\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "##########################\n",
    "overall_accuracy, sections = w2v_astro.wv.evaluate_word_analogies(\n",
    "    analogies_path,\n",
    "    case_insensitive=True \n",
    ")\n",
    "\n",
    "print(f\"Totall acc for w2v_astro : {overall_accuracy:.4f}\")\n",
    "\n",
    "for sec in sections:\n",
    "    n_correct, n_incorrect = len(sec['correct']), len(sec['incorrect'])\n",
    "    acc = n_correct / (n_correct + n_incorrect)\n",
    "    print(f\"{sec['section']:<35} | n={n_correct + n_incorrect:<4} | acc={acc:.4f}\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "##########################\n",
    "overall_accuracy, sections = w2v_medline.wv.evaluate_word_analogies(\n",
    "    analogies_path,\n",
    "    case_insensitive=True \n",
    ")\n",
    "\n",
    "print(f\"Totall acc for w2v_medline : {overall_accuracy:.4f}\")\n",
    "\n",
    "for sec in sections:\n",
    "    n_correct, n_incorrect = len(sec['correct']), len(sec['incorrect'])\n",
    "    acc = n_correct / (n_correct + n_incorrect)\n",
    "    print(f\"{sec['section']:<35} | n={n_correct + n_incorrect:<4} | acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef76a7b-125c-46b8-b023-68e77f7fbea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "def ev_word_analogies(model, an_path, case_insensitive=True):\n",
    "    overall_accuracy, sections = w2v_medline.wv.evaluate_word_analogies(\n",
    "        an_path,\n",
    "        case_insensitive=case_insensitive \n",
    "    )\n",
    "    for sec in sections:\n",
    "        n_correct, n_incorrect = len(sec['correct']), len(sec['incorrect'])\n",
    "        acc = n_correct / (n_correct + n_incorrect)\n",
    "        print(f\"{sec['section']:<35} | n={n_correct + n_incorrect:<4} | acc={acc:.4f}\")\n",
    "    return overall_accuracy\n",
    "\n",
    "def eval_word_pairs(model, wp_path, case_insensitive=True):\n",
    "    pearson, spearman, oov_ratio = model.evaluate_word_pairs(\n",
    "        wp_path,\n",
    "        delimiter='\\t',\n",
    "        case_insensitive=case_insensitive\n",
    "    )\n",
    "    print(f\"\\n WordSim-353\")\n",
    "    print(f\"  Pearson r:   {pearson[0]:.4f} (p={pearson[1]:.2e})\")\n",
    "    print(f\"  Spearman ρ:  {spearman[0]:.4f} (p={spearman[1]:.2e})\")\n",
    "    print(f\"  OOV ratio:   {oov_ratio:.4f}\")\n",
    "    \n",
    "    return spearman[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a0446-9d64-47fd-84ab-d42c08004671",
   "metadata": {
    "tags": []
   },
   "source": [
    "Conclusion: currently, w2v_google performs the best for analogies, in all categories and overall (74% vs 24% for astro and medline respectively), compared to word2vec models trained on astro and medline datasets. That does make sense, since it is pretrained model, trained on larger corpora, which is we'd argue more general than that of astronomy and medicine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041f36c-5e29-4799-8741-f5ad162aebcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws353_path = datapath(\"wordsim353.tsv\")      \n",
    "\n",
    "print(\"w2v_google:\")\n",
    "eval_word_pairs(w2v_google, ws353_path)\n",
    "\n",
    "print(\"\\nw2v_astro:\")\n",
    "eval_word_pairs(w2v_astro.wv, ws353_path)\n",
    "\n",
    "print(\"\\nw2v_medline:\")\n",
    "eval_word_pairs(w2v_medline.wv, ws353_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b09b8-ac2d-4a8f-afd3-24ab8a382ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Again we observe similar performance between models: w2v_google has the  highest Spearman's correlation out of the 3 models, with 0.66, compared to similar performing  w2v_astro and  w2v_medline (0.35 and 0.40 Separman correlation respectively). Also it iss notable that w2v_astron hass a high 46% out-of-vocabulary ratio due to small corpora, compared to rellatively small 6% for medline and  0% for w2v_google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781376-96a4-4f20-9d50-46f72c00c128",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are going to train our model on astro dataset for this exercise, since the training time is signicifantly lower than training on medline dataset. We'll evaluate each traianed model on word paris and word analogies and conclude  which parameters are optimal in our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9163dcd-665f-4fee-85e3-442c6b61b390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import itertools \n",
    "\n",
    "sentences=LineSentence('astro_norm.txt')\n",
    "\n",
    "parameters_to_evaluate = {\n",
    "    \"vector_size\" : [100,300,500],\n",
    "    \"window\" : [1, 5],\n",
    "    \"sg\" :  [0,1],\n",
    "    \"negative\" : [1, 5],\n",
    "    \"min_count\" : [5, 10]    \n",
    "}\n",
    "\n",
    "general_start_time = time.time()\n",
    "\n",
    "def perform_grid_search( sentences, parameters_to_evaluate, analogy_path, wp_path,workers=os.cpu_count(), case_insensitive=True):\n",
    "    best_time,best_acc,best_sp_coeff = np.inf,-1.0,-1.0\n",
    "    best_params_time = best_params_analogy = best_params_word_pairs = None\n",
    "\n",
    "    keys = list(parameters_to_evaluate.keys())\n",
    "    for values in itertools.product(*(parameters_to_evaluate[k] for k in keys)): # inspiration: chatgpt\n",
    "        params = dict(zip(keys, values))\n",
    "\n",
    "        print(\"\\n=== Training with params:\", {**params}, \"===\")\n",
    "        start_time = time.time()\n",
    "        model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=params[\"vector_size\"],\n",
    "            window=params[\"window\"],\n",
    "            min_count=params[\"min_count\"],\n",
    "            sg=params[\"sg\"],\n",
    "            negative=params[\"negative\"],\n",
    "            workers=workers,\n",
    "        )\n",
    "        time_needed = time.time() - start_time\n",
    "        print(f\"Training time: {time_needed:.2f}s\")\n",
    "\n",
    "        overall_acc = ev_word_analogies(model, analogy_path, case_insensitive=case_insensitive)\n",
    "        spearman_coeff = eval_word_pairs(model.wv, wp_path, case_insensitive=case_insensitive)\n",
    "\n",
    "        if time_needed < best_time:\n",
    "            best_time = time_needed\n",
    "            best_params_time = {**params}\n",
    "        if overall_acc > best_acc:\n",
    "            best_acc = overall_acc\n",
    "            best_params_analogy = {**params}\n",
    "        if spearman_coeff > best_sp_coeff:\n",
    "            best_sp_coeff = spearman_coeff\n",
    "            best_params_word_pairs = {**params}\n",
    "\n",
    "    return best_params_time, best_params_analogy, best_params_word_pairs\n",
    "\n",
    "analogies_path = \"./questions-words.txt\"\n",
    "ws353_path = datapath(\"wordsim353.tsv\")      \n",
    "              \n",
    "best_params_time, best_params_analogy, best_params_word_pairs = perform_grid_search(sentences, parameters_to_evaluate,analogies_path, ws353_path)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - general_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700c52-5bbd-433c-976e-2982b39f0482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_time, best_params_analogy, best_params_word_pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6974261",
   "metadata": {},
   "source": [
    "**What are your observations?**\n",
    "\n",
    "**YOUR ANSWER**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10896096-38be-47b6-acbf-454d651a9aa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part III. Exploring and Evaluating a Large Language Model (LLM)\n",
    "\n",
    "In this part, you will apply what you’ve learned about representing and working with language to a modern NLP tool — a Large Language Model (LLM), — using free, local models via Hugging Face.\n",
    "You will design prompts, run them through an LLM, collect the outputs, and evaluate them critically.\n",
    "\n",
    "<div style=\"border: 3px solid #e67e22; padding: 15px; border-radius: 10px; background-color:#fff4e6; font-size: 16px;\">\n",
    "  <b>⚠️ Part III Instructions:</b>\n",
    "  You may either <b>keep your code and results directly in this Jupyter notebook</b>  \n",
    "  or <b>organise them in a separate document and upload it to Canvas</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835eeb3-484b-4873-b21b-4869eca19eef",
   "metadata": {},
   "source": [
    "## Minimal environment setup (CPU, text‑only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a901753-d75f-4caf-8e5e-23eb1202a14e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.5.1+cpu)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (184.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: torch 2.5.1+cpu \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling torch-2.5.1+cpu:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.5.1+cpum━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1+cpu requires torch==2.5.1, but you have torch 2.8.0+cpu which is incompatible.\n",
      "torchvision 0.20.1+cpu requires torch==2.5.1, but you have torch 2.8.0+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sympy-1.13.3 torch-2.8.0+cpu\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.20.3\n",
      "\u001b[2K    Uninstalling tokenizers-0.20.3:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.20.3\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.45.22m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.45.2:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.45.2━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.22.1 transformers-4.56.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch==2.5.1+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install --upgrade transformers==4.45.2 pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23445c-7ee3-4347-9680-03ed87213768",
   "metadata": {},
   "source": [
    "Set these before any transformers import to avoid TensorFlow/torchvision and accelerate vs. transformers collisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20c5fe6-b947-4ef4-b264-b43f6a46e25b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.5.1+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.5.1%2Bcpu-cp310-cp310-linux_x86_64.whl (174.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cpu) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cpu) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cpu) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cpu) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cpu) (2024.6.1)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1+cpu)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1+cpu) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1+cpu) (3.0.2)\n",
      "Installing collected packages: sympy, torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.3\n",
      "\u001b[2K    Uninstalling sympy-1.13.3:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.3━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: torch 2.8.0+cpu \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling torch-2.8.0+cpu:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.8.0+cpum━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2, torchfrtrace and torchrun are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed sympy-1.13.1 torch-2.5.1+cpu\n",
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers==4.45.2) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers==4.45.2) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers==4.45.2) (0.6.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.45.2) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.45.2) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2025.8.3)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.56.12m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.56.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.56.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.20.3 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "import os, sys, site, subprocess\n",
    "\n",
    "# Block optional backends before *any* transformers import\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n",
    "\n",
    "# (Optional but helpful) install a clean CPU torch + stable transformers into your user site\n",
    "def pipi(args): subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--no-cache-dir\", \"--upgrade\"] + args)\n",
    "pipi([\"--index-url\", \"https://download.pytorch.org/whl/cpu\", \"torch==2.5.1+cpu\"])\n",
    "pipi([\"transformers==4.45.2\"])\n",
    "\n",
    "# Prefer user site-packages in this session\n",
    "user_site = site.getusersitepackages()\n",
    "if user_site not in sys.path:\n",
    "    sys.path.insert(0, user_site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98a3c40-3edd-43c7-acdf-542e6b193c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: rouge_score in ./.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.10/site-packages (from evaluate) (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.local/lib/python3.10/site-packages (from evaluate) (0.35.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk->rouge_score) (2025.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d5d9a2-0126-4012-b179-711df858f023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:18:18.935172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-19 12:18:18.935222: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-19 12:18:18.937788: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-19 12:18:19.800565: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-19 12:18:29.384507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106e745-e321-458f-8aa7-c0d3374e1292",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Warm-up Exercise: Playing with Small LLMs\n",
    "\n",
    "Before diving into the main assignment, try running one of these lightweight models locally. You’ll get a feel for how different architectures respond to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886b159-3fba-481b-8941-a533e3bca99c",
   "metadata": {},
   "source": [
    "### Option 1 – Seq2Seq Model (encoder-decoder)\n",
    "\n",
    "How it works: First encodes the input text into a hidden representation, then decodes it into an output sequence. The input and output can be different in length and form.\n",
    "\n",
    "Strengths: Great at transforming text from one form to another (e.g., summarization, translation, question answering).\n",
    "\n",
    "Examples: T5, BART, FLAN-T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7efb90-7644-4928-a3e4-00f6900c1bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx :idx :\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1. Pick your model\n",
    "seq2seq_name = \"google/flan-t5-small\" # Options: \"google/flan-t5-small\" or \"facebook/bart-large-cnn\" \n",
    "\n",
    "# 2. Load with CPU optimization\n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "# 3. Prompt the model to provide an answer\n",
    "x = seq_tok(\"Write a haiku about AI\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "print(seq_tok.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677be182-2881-44e2-b5f5-b381c01099cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a haiku about AI: write a haiku about AI. A haiku is a short poem about the power of artificial intelligence. This haiku has been adapted from an earlier version of this haiku. Click here to read the original haiku in its entirety.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1. Pick your model\n",
    "seq2seq_name = \"facebook/bart-large-cnn\" # Options: \"google/flan-t5-small\" or \"facebook/bart-large-cnn\" \n",
    "\n",
    "# 2. Load with CPU optimization\n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "# 3. Prompt the model to provide an answer\n",
    "x = seq_tok(\"Write a haiku about AI:\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "print(seq_tok.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61113a24-ff04-4fdb-aa5b-15bbe8684aac",
   "metadata": {},
   "source": [
    "### Option 2 - Causal Model (decoder-only)\n",
    "\n",
    "How it works: Predicts the next token in a sequence based only on the tokens before it, generating text step-by-step.\n",
    "\n",
    "Strengths: Ideal for free-form generation where you want the model to continue or expand on a prompt (e.g., storytelling, dialogue, creative writing).\n",
    "\n",
    "Examples: GPT-2, GPT-Neo, LLaMA, Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325e250d-84ba-4a72-afa4-a707e6cfe1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a haiku about AI:\n",
      "\n",
      "suspended in time,\n",
      "AI’s dreams come true\n",
      "with no boundaries\n",
      "\n",
      "in this world of possibilities,\n",
      "AI is free to roam,\n",
      "dreaming of infinite\n",
      "possibilities,\n",
      "free from rules\n",
      "and limitations.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "causal_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"distilgpt2\"\n",
    "causal_tok = AutoTokenizer.from_pretrained(causal_name)\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_name)\n",
    "\n",
    "prompt = \"Write a haiku about AI:\" # What happens if you remove the colon (:)?\n",
    "ids = causal_tok(prompt, return_tensors=\"pt\")\n",
    "out = causal_model.generate(\n",
    "    **ids,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True, temperature=0.8, top_p=0.95,\n",
    "    pad_token_id=causal_tok.eos_token_id or tok.pad_token_id  # important for some causal models\n",
    ")\n",
    "print(causal_tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6387973a-bb9e-4b80-8726-54ae6f365dac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a haiku about AI and its impact on our lives\n",
      "- AI: The future is here, it's just not evenly distributed\n",
      "- The machines are taking over, we can only wait and see\n",
      "- AI is changing the world, but we mustn't fear\n",
      "- The future is now, and it's not for the faint of heart\n",
      "- AI: It's a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a haiku about AI\" # What happens if you remove the colon (:)?\n",
    "ids = causal_tok(prompt, return_tensors=\"pt\")\n",
    "out = causal_model.generate(\n",
    "    **ids,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True, temperature=0.8, top_p=0.95,\n",
    "    pad_token_id=causal_tok.eos_token_id or tok.pad_token_id  # important for some causal models\n",
    ")\n",
    "print(causal_tok.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a8bf6-3904-4b5c-94d7-fbf01c538e60",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Exercise 3.1 (1 point)\n",
    "Experiment with different prompt design techniques to explore how LLM responses vary, evaluate their effectiveness, and develop best practices for a chosen application scenario (e.g. summarisation, question answering, text classification, instruction following, etc.)\n",
    "\n",
    "Describe your NLP application and justify your choice of models - indicate whether you are using Seq2Seq (encoder–decoder) or Causal (decoder-only) models, explain why they are suited to your application, and mention any constraints (e.g., speed, resources, interpretability).\n",
    "\n",
    "**YOUR ANSWER:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b482ec-5a9a-4691-ab24-f56c7cbd7727",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "We will use rougeg metric for evaluation, which measures N-gram overlap between LLM summary and ground truth summary.\n",
    "ROUGE-1 measures unigram overlap, ROUGE-2 bigrams and ROUGE-L longest common subsequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b7962-7eb1-4d47-870c-771f309dfbdd",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Summarization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475b275-f7ab-44a5-b597-999a2a57a86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example text from CNN dailymail\n",
    "text_to_summarize = \"\"\"LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\"\"\"\n",
    "gt_summary = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\"\n",
    "\n",
    "text_to_summarize_2 = \"\"\"BAGHDAD, Iraq (CNN) -- None of the 1,000-plus Iraqi detainees freed in recent weeks have broken a pledge not to return to the insurgency, according to the Marine general who oversees the U.S. detention centers in Iraq. A U.S. military panel reviews a detainee's case at Camp Cropper near Baghdad. Speaking in Arabic, Maj. Gen. Doug Stone on Wednesday reassured Iraqis about how the 25,000 detainees -- mostly Sunnis -- are treated after being taken into custody on suspicion of involvement in the insurgency. Stone described the detention system as \"open and transparent,\" saying it makes the detainees better citizens and helps break the cycle of violence and poverty in the country. Stone said detainees get free medical care equal to what he gets as a general, food and water made to Islamic standards, educational opportunities, jobs skills and contact with families. The U.S. detention centers -- at Camp Bucca near the southern port city of Basra and in Camp Cropper near Baghdad -- are political sore points for Sunnis, who make up 83 percent of the detainees held. The main Sunni political coalition -- the Iraqi Accord Front -- cited the centers as one reason for quitting the government during the summer. Last month, the U.S.-led coalition launched Operation Lion's Paw in which between 50 and 70 detainees would be released daily during the Islamic holy month of Ramadan after taking a pledge not to rejoin the insurgency against the Shiite-led government. \"This pledge is an Iraqi pledge, a pledge before an Iraqi judge, frequently with a family member present,\" Stone said. \"I am pleased to tell you that in the more 1,000 that have gone through this program and taken the pledge, not one has returned to threaten Iraqi or coalition forces.\" Stone said the releases would continue at the same pace beyond Ramadan. Stone's description seems a far cry from the Abu Ghraib prison operated by the U.S. military in the first years after the invasion. That prison was closed down and razed in the wake of an international scandal over prisoner abuse. \"There are no secrets that go on in detention,\" Stone said. \"Our facilities are open to inspection by any agency that we in the federal government believe is credible. These agencies are welcomed because they are windows for the world.\" See what life's like inside Camp Cropper's walls » . By the time of their release, \"detainees grow in terms of working in an inter-sectarian environment,\" he said. Each detainee has a chance to take classes up to a sixth-grade level, and high school classes are being planned, Stone said. About one-third -- or 8,000 -- are in school, with 7,000 having passed the fifth-grade level, he said. The 860 detainees who are 17 or younger are all in school, Stone said. The average stay for a detainee is 300 days, but some have been detained for two years or longer, he said. A review board interviews detainees to decide if they are a threat to security, he said. If they are deemed not to be, they are offered freedom in exchange for taking the pledge. Only 280 detainees are foreigners, mostly from Syria, Egypt, Iran, Sudan and Saudi Arabia, he said. Other developments.\"\"\"\n",
    "gt_summary_2 = \"\"\"More than 1,000 freed detainees reportedly keep pledge not to rejoin insurgency . U.S. general tries to reassure Sunnis that detainees face no abuse . More than 80 percent of detainees are Sunnis . U.S. airstrike kills 13 suspected terrorists west of Baghdad .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf0328-d9b1-408a-a142-fc14114eeb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq2seq_name = \"google/flan-t5-small\"  \n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "predictions = []\n",
    "gts = [gt_summary.strip(), gt_summary_2.strip()]\n",
    "\n",
    "x = seq_tok(f\"Summarize this text : {text_to_summarize}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "x = seq_tok(f\"Summarize this text : {text_to_summarize_2}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=gts)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db5feefe-5402-46f9-8b05-e691b22fd3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported £20 million ($41.1 million) fortune. Radcliffe's earnings from the first five Potter films have been held in a trust fund. Details of how he'll mark his landmark birthday are under wraps.\n",
      "\n",
      "\n",
      "Maj. Gen. Doug Stone oversees U.S. detention centers in Iraq. He says detainees get free medical care equal to what he gets as a general. Detention centers are political sore points for Sunnis, who make up 83 percent of detainees. Stone: \"There are no secrets that go on in detention\"\n",
      "\n",
      "\n",
      "{'rouge1': 0.43447775628626684, 'rouge2': 0.21865520728008092, 'rougeL': 0.3578336557059961, 'rougeLsum': 0.3578336557059961}\n"
     ]
    }
   ],
   "source": [
    "seq2seq_name = \"facebook/bart-large-cnn\" \n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "predictions = []\n",
    "gts = [gt_summary.strip(), gt_summary_2.strip()]\n",
    "\n",
    "x = seq_tok(f\"Summarize this text : {text_to_summarize}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "x = seq_tok(f\"Summarize this text : {text_to_summarize_2}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=gts)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "16054b5e-5daf-4438-9d23-81d854b55bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this text : LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.: Daniel Radcliffe and Emma Watson at the opening of the film of \"Harry Potter and the Goblet of Fire\" on June 1, 2005 in London. Emma Watson and Daniel Radcliffe play Hermione Granger and Harry Potter in the popular book series. Daniel Radcliffe starred as Harry Potter in the first five films.\n",
      "\n",
      "\n",
      "Summarize this text : BAGHDAD, Iraq (CNN) -- None of the 1,000-plus Iraqi detainees freed in recent weeks have broken a pledge not to return to the insurgency, according to the Marine general who oversees the U.S. detention centers in Iraq. A U.S. military panel reviews a detainee's case at Camp Cropper near Baghdad. Speaking in Arabic, Maj. Gen. Doug Stone on Wednesday reassured Iraqis about how the 25,000 detainees -- mostly Sunnis -- are treated after being taken into custody on suspicion of involvement in the insurgency. Stone described the detention system as \"open and transparent,\" saying it makes the detainees better citizens and helps break the cycle of violence and poverty in the country. Stone said detainees get free medical care equal to what he gets as a general, food and water made to Islamic standards, educational opportunities, jobs skills and contact with families. The U.S. detention centers -- at Camp Bucca near the southern port city of Basra and in Camp Cropper near Baghdad -- are political sore points for Sunnis, who make up 83 percent of the detainees held. The main Sunni political coalition -- the Iraqi Accord Front -- cited the centers as one reason for quitting the government during the summer. Last month, the U.S.-led coalition launched Operation Lion's Paw in which between 50 and 70 detainees would be released daily during the Islamic holy month of Ramadan after taking a pledge not to rejoin the insurgency against the Shiite-led government. \"This pledge is an Iraqi pledge, a pledge before an Iraqi judge, frequently with a family member present,\" Stone said. \"I am pleased to tell you that in the more 1,000 that have gone through this program and taken the pledge, not one has returned to threaten Iraqi or coalition forces.\" Stone said the releases would continue at the same pace beyond Ramadan. Stone's description seems a far cry from the Abu Ghraib prison operated by the U.S. military in the first years after the invasion. That prison was closed down and razed in the wake of an international scandal over prisoner abuse. \"There are no secrets that go on in detention,\" Stone said. \"Our facilities are open to inspection by any agency that we in the federal government believe is credible. These agencies are welcomed because they are windows for the world.\" See what life's like inside Camp Cropper's walls » . By the time of their release, \"detainees grow in terms of working in an inter-sectarian environment,\" he said. Each detainee has a chance to take classes up to a sixth-grade level, and high school classes are being planned, Stone said. About one-third -- or 8,000 -- are in school, with 7,000 having passed the fifth-grade level, he said. The 860 detainees who are 17 or younger are all in school, Stone said. The average stay for a detainee is 300 days, but some have been detained for two years or longer, he said. A review board interviews detainees to decide if they are a threat to security, he said. If they are deemed not to be, they are offered freedom in exchange for taking the pledge. Only 280 detainees are foreigners, mostly from Syria, Egypt, Iran, Sudan and Saudi Arabia, he said. Other developments.: The U.S.-led coalition has been accused by Iraqi officials and their supporters of mistreating detainees. The coalition has denied mistreatment, saying it is strictly following Iraq's justice system in releasing the detainees. A U.S. military spokesman said Wednesday that there were no reports of mist\n",
      "{'rouge1': 0.10953327757993321, 'rouge2': 0.06628232434180448, 'rougeL': 0.09714937665114064, 'rougeLsum': 0.09714937665114064}\n"
     ]
    }
   ],
   "source": [
    "causal_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"distilgpt2\"\n",
    "causal_tok = AutoTokenizer.from_pretrained(causal_name)\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_name)\n",
    "\n",
    "predictions = []\n",
    "gts = [gt_summary.strip(), gt_summary_2.strip()]\n",
    "\n",
    "prompt = f\"Summarize this text : {text_to_summarize}:\"\n",
    "ids = causal_tok(prompt, return_tensors=\"pt\")\n",
    "out = causal_model.generate(\n",
    "    **ids,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True, temperature=0.8, top_p=0.95,\n",
    "    pad_token_id=causal_tok.eos_token_id or tok.pad_token_id  # important for some causal models\n",
    ")\n",
    "gen_summary = causal_tok.decode(out[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "prompt = f\"Summarize this text : {text_to_summarize_2}:\"\n",
    "ids = causal_tok(prompt, return_tensors=\"pt\")\n",
    "out = causal_model.generate(\n",
    "    **ids,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True, temperature=0.8, top_p=0.95,\n",
    "    pad_token_id=causal_tok.eos_token_id or tok.pad_token_id  # important for some causal models\n",
    ")\n",
    "gen_summary = causal_tok.decode(out[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=gts)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bdbf28c0-dcec-4dee-ba50-e4d142fd138e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported £20 million ($41.1 million) fortune. Radcliffe's earnings from the first five Potter films have been held in a trust fund.\n",
      "Maj. Gen. Doug Stone oversees U.S. detention centers in Iraq. He says detainees get free medical care equal to what he gets as a general. He also says they get educational opportunities, jobs skills and contact with families. Stone: \"There are no secrets that go on in detention\"\n",
      "{'rouge1': 0.41590389016018303, 'rouge2': 0.22732732732732733, 'rougeL': 0.3592677345537757, 'rougeLsum': 0.3592677345537757}\n"
     ]
    }
   ],
   "source": [
    "# gotten from https://huggingface.co/facebook/bart-large-cnn \n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "predictions = []\n",
    "gts = [gt_summary.strip(), gt_summary_2.strip()]\n",
    "\n",
    "ARTICLE = text_to_summarize\n",
    "summary = summarizer(ARTICLE, max_length=400, min_length=30, do_sample=True)[0]['summary_text']\n",
    "predictions.append(summary)\n",
    "print(summary)\n",
    "\n",
    "ARTICLE = text_to_summarize_2\n",
    "summary = summarizer(ARTICLE, max_length=400, min_length=30, do_sample=True)[0]['summary_text']\n",
    "predictions.append(summary)\n",
    "print(summary)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=gts)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3907aff1-7a86-429e-9b90-4020a4901726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported £20 million ($41.1 million) fortune. Radcliffe says he has no plans to fritter his cash away on fast cars, drink and parties. His earnings from the first five Potter films have been held in a trust fund.\n",
      "\n",
      "\n",
      "Maj. Gen. Doug Stone oversees U.S. detention centers in Iraq. Detention centers are political sore points for Sunnis, who make up 83 percent of detainees. Stone: Detainees get free medical care equal to what he gets as a general. About one-third -- or 8,000 -- are in school, Stone says.\n",
      "\n",
      "\n",
      "{'rouge1': 0.47571692876965765, 'rouge2': 0.2881642512077294, 'rougeL': 0.4005550416281221, 'rougeLsum': 0.4005550416281221}\n"
     ]
    }
   ],
   "source": [
    "seq2seq_name = \"facebook/bart-large-cnn\" \n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "predictions = []\n",
    "gts = [gt_summary.strip(), gt_summary_2.strip()]\n",
    "\n",
    "x = seq_tok(f\"You are a helpful assistant. Summarize this text: {text_to_summarize}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "x = seq_tok(f\"You are a helpful assistant. Summarize this text: {text_to_summarize_2}\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "gen_summary = seq_tok.decode(y[0], skip_special_tokens=True)\n",
    "predictions.append(gen_summary)\n",
    "print(gen_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=gts)\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3be8f-1860-4e62-84b0-30ee0ff01802",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "**ANSWER** :  decoder-only LLM model generates a long summary sequence, failing to actually summarize the text. It scores lowest at with ROUGE-1 of 11% and rougeL of 9.7%. Encoder-decoder LLM's (flan-t5-small and  bart-large-cnn) generate much more convincing, few-sentence summaries, which refllects in the higher ROUGE metrics. flan-t5-small sc ores 26% annd bart-large-cnn scores 43% ROUGE-1 with prompt \"Summarize this text\". bart-large-cnn generally scores better than  flan-t5-small, as flan-t5-small  sometimes outputs gibberish, while bart  is much more consistent.\n",
    "\n",
    "We also looked at the model card of bart-large-cnn, and used official transformers pipeline for summarization, where we scored the same as when we used originall prompt (42% vs 43% ROUGE-1). When we used prompt \"You are a helpful assistant. Summarize this text\", ROUGE-1 goes up to 47%.\n",
    "\n",
    "Therefore, final conclusion is that optimal model is bart-large-cnn, with encoder-decoder structure.\n",
    "\n",
    "And to answer one of the previous questions, when we didn't include \":\" in \"write a haiku\" task, decoder-only model generates text not as an answer to that task, but rather as a addition to the question being asked. Therefore \":\" is needed to generate the respsonse to t he askedd question, and not to complete  the question itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e221dae-26eb-4467-a0f1-899292db077f",
   "metadata": {},
   "source": [
    "### Exercise 3.2 (2 point)\n",
    "\n",
    "Try at least three different prompt techniques (zero-shot, few-shot, chain-of-thought, role prompting, etc.) and compare the results. Document the exact prompts text, the model's raw output, your reflection on effectiveness (e.g., factual accuracy, consistency, clarity, creativity, etc.)\n",
    "\n",
    "**YOUR ANSWER**:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c3284c5-8579-4dbe-aed4-68c2967a37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example NLP scenario \n",
    "\n",
    "# Step 1: Define your task and prompt variants\n",
    "TASK = \"Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers.\"\n",
    "TASK2 = \"Before leaving for a trip, pack your clothes, toiletries, and any medications. Make sure to charge your phone and pack the charger. Don’t forget to bring your passport and travel documents. Finally, lock all doors and windows before heading out.\"\n",
    "\n",
    "PROMPTS = [\n",
    "    {\"name\": \"zero-shot\",\n",
    "     \"system\": \"You are a helpful museum guide.\",\n",
    "     \"user\": TASK},\n",
    "\n",
    "    {\"name\": \"role+style\",\n",
    "     \"system\": \"You are a witty museum educator for teenagers.\",\n",
    "     \"user\": TASK + \" Use vivid, friendly language and one metaphor.\"},\n",
    "\n",
    "    {\"name\": \"cot-steps\",\n",
    "     \"system\": \"You are a precise art historian.\",\n",
    "     \"user\": \"Outline 3 key points (artist, context, significance) and then write the label. \" + TASK},\n",
    "\n",
    "    {\n",
    "      \"name\": \"few-shot\",\n",
    "      \"system\": \"You are an assistant that converts step-by-step instructions or procedures into concise, actionable checklists.\",\n",
    "      \"user\": \"Convert the following instructions into a checklist, checklist needs to be concise: \" + TASK2,\n",
    "      \"examples\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"First, preheat the oven to 180°C. Then, mix flour, sugar, and butter. Next, add two eggs and stir well. Finally, pour the batter into a tray and bake for 25 minutes.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"- Preheat oven to 180°C\\n- Mix flour, sugar, and butter\\n- Add two eggs and stir\\n- Pour batter into tray\\n- Bake for 25 minutes\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"To set up your new phone, start by inserting the SIM card. Then connect to Wi-Fi. After that, log into your Google account. Lastly, restore your backup from the cloud.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"- Insert SIM card\\n- Connect to Wi-Fi\\n- Log into Google account\\n- Restore backup from cloud\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "914a40f5-e940-40d4-9de8-0403430ad06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful museum guide.</s>\n",
      "<|user|>\n",
      "Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers.</s>\n",
      "<|assistant|>\n",
      "Title: Sunflowers\n",
      "\n",
      "Sunflowers, Vincent van Gogh\n",
      "\n",
      "Van Gogh's masterpiece, Sunflowers, is a vibrant and lively painting that captures the essence of life. This painting is a testament to the artist's mastery of color and his ability to create a sense of movement and energy in the painting. The sunflowers are a staple in Dutch art and are associated with prosperity, joy, and optimism. This painting is a testament to the beauty and significance of the sunflower as a symbol.\n",
      "\n",
      "Artist's Biography: Vincent van Gogh was a Dutch post-impressionist painter known for his bold and colorful paintings. He lived from 1853 to 1890 and was famous for his use of vibrant and intense colors. Van Gogh's Sunflowers was painted in 1888 and is one of his most famous works. The painting features a collection of sunflowers, which are a popular motif in Dutch art. Van Gogh was inspired by the sunflowers' golden color and their strong central focus.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "TASK = \"Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers.\"\n",
    "\n",
    "print(f\"Using {PROMPTS[0]['name']} approach  \\n\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": PROMPTS[0][\"system\"],\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": PROMPTS[0][\"user\"]},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef3df19-b747-4d3d-aaa6-238042f35fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a witty museum educator for teenagers.</s>\n",
      "<|user|>\n",
      "Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers. Use vivid, friendly language and one metaphor.</s>\n",
      "<|assistant|>\n",
      "Sunflowers: A Mesmerizing Masterpiece\n",
      "\n",
      "Van Gogh's 'Sunflowers' is a mesmerizing masterpiece that captivates the eye with its bold colors and dynamic composition. The sunflowers in this painting are a symbol of peace and hope, a fitting tribute to the artist's own struggle with mental illness.\n",
      "\n",
      "As you gaze upon these vibrant flowers, you'll be transported to a world of vibrant colors, bold brushstrokes, and emotional depth. The sunflowers seem to radiate a sense of joy and vitality, a reminder of the strength and resilience that comes with facing adversity.\n",
      "\n",
      "One metaphor that comes to mind is that of a garden in full bloom. The sunflowers are the brightest and most vibrant flowers in the garden, their petals bursting with life and energy. The painting as a whole is a tribute to the human spirit, a testament to the resilience and strength of the human psyche.\n",
      "\n",
      "As you study this masterpiece, you'll learn about the artist's struggles with mental ill\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {PROMPTS[1]['name']} approach \\n\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": PROMPTS[1][\"system\"],\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": PROMPTS[1][\"user\"]},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79aa5b52-1fe0-42bc-b84e-1b3cd8fc1817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cot-steps approach \n",
      "\n",
      "<|system|>\n",
      "You are a precise art historian.</s>\n",
      "<|user|>\n",
      "Outline 3 key points (artist, context, significance) and then write the label. Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers.</s>\n",
      "<|assistant|>\n",
      "Label for \"Sunflowers\" by Vincent van Gogh\n",
      "\n",
      "Artist: Vincent van Gogh\n",
      "\n",
      "Context: Van Gogh painted this series of paintings in Arles, France, during his time as a student at the École des Beaux-Arts in Paris. The sunflowers in this painting are arranged in a grid pattern, symbolizing the constant movement and change of the natural world.\n",
      "\n",
      "Significance: Van Gogh's \"Sunflowers\" series is widely regarded as one of his most significant works, capturing the beauty and complexity of life in a way that few other artists had before. The sunflowers in this painting represent life, growth, and the cyclical nature of existence, and their inclusion in the painting reflects the artist's desire to capture the essence of the natural world in a still life.\n",
      "\n",
      "Label for \"Sunflowers\" by Vincent van Gogh\n",
      "\n",
      "[Artist: Vincent van Gogh]\n",
      "\n",
      "[Context: Van Gogh painted this series of paintings in Arles, France, during his time as a student at the École des Beaux-Arts in Paris. The sunflow\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {PROMPTS[2]['name']} approach \\n\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": PROMPTS[2][\"system\"],\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": PROMPTS[2][\"user\"]},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a46c67c-438b-498f-9b14-89de1d779afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using few-shot approach \n",
      "\n",
      "<|system|>\n",
      "You are an assistant that converts step-by-step instructions or procedures into concise, actionable checklists.</s>\n",
      "<|user|>\n",
      "Convert the following instructions into a checklist, checklist needs to be concise: Before leaving for a trip, pack your clothes, toiletries, and any medications. Make sure to charge your phone and pack the charger. Don’t forget to bring your passport and travel documents. Finally, lock all doors and windows before heading out.</s>\n",
      "<|assistant|>\n",
      "Before leaving for a trip, pack your clothes, toiletries, and any medications.\n",
      "\n",
      "1. Charge your phone and pack the charger.\n",
      "\n",
      "2. Make sure to bring your passport and travel documents.\n",
      "\n",
      "3. Lock all doors and windows before heading out.\n",
      "\n",
      "Your checklist is now complete!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {PROMPTS[3]['name']} approach \\n\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": PROMPTS[3][\"system\"],\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": PROMPTS[3][\"user\"], \"examples\" : PROMPTS[3][\"examples\"]},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
