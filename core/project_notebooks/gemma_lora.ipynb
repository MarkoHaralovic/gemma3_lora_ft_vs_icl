{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2769b-08b2-4901-8f01-40476277070e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"          \n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fc48f-dc53-463e-8e06-07f6859211d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0706ad-37c5-4680-b4ba-8e3f3313637b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d741e0-9a13-4189-9ae7-4f37f18c8709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "four_bit_quantization=False\n",
    "eight_bit_quantization=False\n",
    "\n",
    "full_models_to_finentune = {\n",
    "    \"google/gemma-3-270m-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 128,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 128,         \n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True, \n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"r\": 128,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 128,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": False,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    },\n",
    "    \"google/gemma-3-1b-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 64,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 64,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"r\": 64,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 64,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": False,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    },\n",
    "    \"google/gemma-3-4b-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 32,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 32,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"r\": 32,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 32,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    },\n",
    "    \"google/gemma-3-12b-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 16,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.1,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"r\": 16,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.1,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    },\n",
    "    \"google/gemma-3-27b-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 16,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.1,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "        \"question_answering\": {\n",
    "            \"r\": 16,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.1,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True,\n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "models_configs = {\n",
    "    \"google/gemma-3-270m-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        max_steps=500,                \n",
    "        learning_rate=5e-5,          \n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-270m-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-1b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        max_steps=500,\n",
    "        learning_rate=3e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-1b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-4b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,   \n",
    "        gradient_accumulation_steps=2,   \n",
    "        warmup_steps=50,                \n",
    "        max_steps=1000,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-4b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-12b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=1200,\n",
    "        learning_rate=1.5e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-12b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-27b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=1200,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-27b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614b5ea-5805-42d4-89f9-e1fff9828a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "four_bit_quantization=False\n",
    "eight_bit_quantization=False\n",
    "\n",
    "models_to_finentune = {\n",
    "    \"google/gemma-3-1b-it\": {\n",
    "        \"classification\": {\n",
    "            \"r\": 64,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"lora_alpha\": 64,\n",
    "            \"lora_dropout\": 0.10,\n",
    "            \"bias\": \"none\",\n",
    "            \"use_gradient_checkpointing\": True, \n",
    "            \"use_rslora\": False,\n",
    "            \"loftq_config\": None\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models_configs = {\n",
    "    \"google/gemma-3-270m-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        max_steps=500,                \n",
    "        learning_rate=5e-5,          \n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-270m-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-1b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        max_steps=500,\n",
    "        learning_rate=3e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-1b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-4b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,   \n",
    "        gradient_accumulation_steps=2,   \n",
    "        warmup_steps=50,                \n",
    "        max_steps=1000,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-4b-it\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    \"google/gemma-3-12b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,\n",
    "        max_steps=600,\n",
    "        learning_rate=1.5e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-12b-it\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    "    \"google/gemma-3-27b-it\": SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=100,\n",
    "        max_steps=500,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/gemma-3-27b-it\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a14a5f-7483-4d5f-9314-b047b15004a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_chatml(example):\n",
    "    if \"question\" in example and \"answers\" in example:\n",
    "        if isinstance(example[\"answers\"], dict):\n",
    "            if \"text\" in example[\"answers\"]:\n",
    "                answer_text = example[\"answers\"][\"text\"][0] if len(example[\"answers\"][\"text\"]) > 0 else \"\"\n",
    "            elif \"value\" in example[\"answers\"]:\n",
    "                answer_text = example[\"answers\"][\"value\"]\n",
    "            else:\n",
    "                answer_text = \"\"\n",
    "        else:\n",
    "            answer_text = str(example[\"answers\"])\n",
    "\n",
    "        user_msg = f\"Answer the following question based on the context:\\n\\n{example.get('context', '')}\\n\\nQuestion: {example['question']}\"\n",
    "        system_prompt = \"You are a knowledgeable assistant that answers factual questions.\"\n",
    "\n",
    "    elif \"text\" in example and \"label\" in example:\n",
    "        label = example.get(\"label_names\", \"\") if \"label_names\" in example else example.get(\"label\",\"\")\n",
    "        user_msg = f\"Classify the sentiment or topic of the following text:\\n\\n{example['text']}\\n\\nLabel:\"\n",
    "        system_prompt = \"You are a helpful assistant that performs text classification tasks.\"\n",
    "        if isinstance(label, int) and \"label_names\" in example:\n",
    "            answer_text = example[\"label_names\"][label]\n",
    "        else:\n",
    "            answer_text = str(label)\n",
    "    else:\n",
    "        user_msg = example.get(\"question\", example.get(\"text\", \"\"))\n",
    "        print(user_msg)\n",
    "        if isinstance(label, int) and label_names is not None:\n",
    "            answer_text = label_names[label] \n",
    "        else:\n",
    "            answer_text = str(label)\n",
    "        system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": answer_text},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, tokenizer, train_samples=None, eval_samples=None):\n",
    "    if train_samples is not None:\n",
    "        train_dataset = load_dataset(dataset_name, split=f\"train[:{train_samples}]\")\n",
    "    else:\n",
    "        train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    try:\n",
    "        if eval_samples is not None:\n",
    "            eval_dataset = load_dataset(dataset_name, split=f\"test[:{eval_samples}]\")\n",
    "        else:\n",
    "            eval_dataset = load_dataset(dataset_name, split=\"test[:500]\")  \n",
    "    except:\n",
    "        total_samples = len(train_dataset)\n",
    "        eval_size = min(500, total_samples // 10)\n",
    "        eval_dataset = train_dataset.select(range(eval_size))\n",
    "        train_dataset = train_dataset.select(range(eval_size, total_samples))\n",
    "        \n",
    "    if dataset_name ==\"ag_news\":\n",
    "        label_names = train_dataset.features[\"label\"].names\n",
    "        train_dataset = train_dataset.map(\n",
    "            lambda itm : {\"label_names\" : label_names[itm['label']]}\n",
    "        )\n",
    "\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            lambda itm : {\"label_names\" : label_names[itm['label']]}\n",
    "        )\n",
    "    \n",
    "    train_dataset_chatml = train_dataset.map(convert_to_chatml)\n",
    "    eval_dataset_chatml = eval_dataset.map(convert_to_chatml)\n",
    "    \n",
    "    def formatting_prompts_func(examples):\n",
    "        convos = examples[\"conversations\"]\n",
    "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix(\"<bos>\") for convo in convos]\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    train_dataset_formatted = train_dataset_chatml.map(formatting_prompts_func, batched=True)\n",
    "    eval_dataset_formatted = eval_dataset_chatml.map(formatting_prompts_func, batched=True)\n",
    "    \n",
    "    return train_dataset_formatted, eval_dataset_formatted  # FIXED: Return both train and eval\n",
    "\n",
    "def get_dataset(task,tokenizer, dataset_name=None,train_samples=10000, eval_samples=500):\n",
    "    if task == \"classification\":\n",
    "        dataset_name = \"ag_news\" if dataset_name is None else dataset_name\n",
    "        return prepare_dataset(dataset_name,tokenizer, train_samples,eval_samples), \"ag_news\"  \n",
    "    elif task == \"question_answering\":\n",
    "        dataset_name = \"squad_v2\" if dataset_name is None else dataset_name\n",
    "        return prepare_dataset(dataset_name,tokenizer, train_samples, eval_samples), \"squad_v2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc9694-efe9-4f0d-93f4-41c73fa02320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv, os, time\n",
    "\n",
    "os.makedirs(\"lora_results\", exist_ok=True)\n",
    "CSV_PATH = \"lora_results/lora_finetune_metrics_v2.csv\"\n",
    "\n",
    "def _append_row_csv(path, row: dict):\n",
    "    file_exists = os.path.exists(path)\n",
    "    with open(path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if not file_exists:writer.writeheader()\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06463b74-a53f-435c-bdc3-dcda4b7a1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    import os\n",
    "    os.system(\"\"\"\n",
    "    echo \"Cleaning up vLLM and CUDA contexts\"\n",
    "    pkill -f \"vllm\" || true\n",
    "    pkill -f \"engine_core\" || true\n",
    "    pkill -f \"torchrun\" || true\n",
    "    sleep 2\n",
    "    fuser -k /dev/nvidia* || true\n",
    "    \"\"\")\n",
    "clean_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a16f41-8dd5-47b0-8f9b-22240f2a5d61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, task_lora in models_to_finentune.items(): \n",
    "    for task, lora_parameters in task_lora.items(): \n",
    "        print(f\"\\n=== Training {model_name} on {task} ===\")\n",
    "        model, tokenizer = FastModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=four_bit_quantization,\n",
    "            load_in_8bit=eight_bit_quantization,\n",
    "            full_finetuning=False,\n",
    "            device_map={'':torch.cuda.current_device()},\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=0.90, \n",
    "        )\n",
    "        \n",
    "        model = FastModel.get_peft_model(model, **lora_parameters)\n",
    "        tokenizer = get_chat_template(tokenizer, chat_template=\"gemma3\")\n",
    "        \n",
    "        datasets, dn = get_dataset(task=task,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   train_samples=10000, \n",
    "                                   eval_samples=500\n",
    "                                  )\n",
    "        train_dataset, eval_dataset = datasets\n",
    "    \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "        print(f\"LoRA parameters: r={lora_parameters['r']}, alpha={lora_parameters['lora_alpha']}\")\n",
    "              \n",
    "        cfg = models_configs[model_name]\n",
    "\n",
    "        cfg.do_eval = True\n",
    "        cfg.evaluation_strategy = \"steps\"   # HF/TRL name\n",
    "        cfg.eval_strategy = \"steps\"         # Unsloth alias shown in your printout\n",
    "        cfg.eval_steps = 100\n",
    "        cfg.logging_strategy = \"steps\"\n",
    "        cfg.logging_steps = 50\n",
    "        cfg.eval_on_start = True            # optional: see an eval at step 0\n",
    "        cfg.load_best_model_at_end = True   # optional: keep the best checkpoint\n",
    "        cfg.metric_for_best_model = \"eval_loss\"\n",
    "        cfg.greater_is_better = False\n",
    "\n",
    "        cfg.bf16 = True \n",
    "        cfg.fp16 = False\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,                             \n",
    "            eval_dataset=eval_dataset, \n",
    "            args=cfg,\n",
    "        )\n",
    "\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part=\"<start_of_turn>user\\n\",\n",
    "            response_part=\"<start_of_turn>model\\n\",\n",
    "        )\n",
    "\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "        trainer_stats = trainer.train()\n",
    "\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "        \n",
    "        print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "        \n",
    "        row = {\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model\": model_name,\n",
    "            \"task\": task,\n",
    "            \"dataset\": dn,  \n",
    "            \"run_dir\": models_configs[model_name].output_dir,\n",
    "\n",
    "            \"r\": lora_parameters.get(\"r\"),\n",
    "            \"lora_alpha\": lora_parameters.get(\"lora_alpha\"),\n",
    "            \"lora_dropout\": lora_parameters.get(\"lora_dropout\"),\n",
    "            \"bias\": lora_parameters.get(\"bias\"),\n",
    "            \"target_modules\": \"|\".join(lora_parameters.get(\"target_modules\", [])),\n",
    "            \"use_gradient_checkpointing\": lora_parameters.get(\"use_gradient_checkpointing\"),\n",
    "            \"use_rslora\": lora_parameters.get(\"use_rslora\"),\n",
    "            \"loftq_config\": bool(lora_parameters.get(\"loftq_config\")),\n",
    "\n",
    "\n",
    "            \"max_seq_length\": max_seq_length,\n",
    "            \"four_bit_quantization\": bool(four_bit_quantization),\n",
    "            \"eight_bit_quantization\": bool(eight_bit_quantization),\n",
    "\n",
    "            \"learning_rate\": models_configs[model_name].learning_rate,\n",
    "            \"per_device_train_batch_size\": models_configs[model_name].per_device_train_batch_size,\n",
    "            \"gradient_accumulation_steps\": models_configs[model_name].gradient_accumulation_steps,\n",
    "            \"warmup_steps\": models_configs[model_name].warmup_steps,\n",
    "            \"max_steps\": models_configs[model_name].max_steps,\n",
    "            \"weight_decay\": models_configs[model_name].weight_decay,\n",
    "            \"optim\": models_configs[model_name].optim,\n",
    "            \"lr_scheduler_type\": getattr(models_configs[model_name], \"lr_scheduler_type\", None),\n",
    "\n",
    "            \"train_runtime_sec\": trainer_stats.metrics.get(\"train_runtime\"),\n",
    "            \"train_samples_per_sec\": trainer_stats.metrics.get(\"train_samples_per_second\"),\n",
    "            \"train_steps_per_sec\": trainer_stats.metrics.get(\"train_steps_per_second\"),\n",
    "            \"global_step\": trainer_stats.metrics.get(\"global_step\"),\n",
    "            \"train_loss\": trainer_stats.metrics.get(\"train_loss\"),\n",
    "            \"epoch\": trainer_stats.metrics.get(\"epoch\"),\n",
    "\n",
    "            \"gpu_name\": gpu_stats.name,\n",
    "            \"gpu_total_gb\": max_memory,\n",
    "            \"gpu_reserved_start_gb\": start_gpu_memory,\n",
    "            \"gpu_reserved_peak_gb\": used_memory,\n",
    "            \"gpu_reserved_train_gb\": used_memory_for_lora,\n",
    "            \"gpu_reserved_peak_pct\": used_percentage,\n",
    "            \"gpu_reserved_train_pct\": lora_percentage,\n",
    "        }\n",
    "\n",
    "        _append_row_csv(CSV_PATH, row)\n",
    "        print(f\"[metrics] appended row to {CSV_PATH}\")\n",
    "\n",
    "        run_id = f\"{model_name.replace('/', '_')}_ft_{dn}_v3\"\n",
    "        os.makedirs(\"models\", exist_ok=True)                    \n",
    "        model.save_pretrained(f\"models/{run_id}\")\n",
    "        tokenizer.save_pretrained(f\"models/{run_id}\")\n",
    "\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        model.push_to_hub(f\"Mhara/{run_id}\", token=hf_token)\n",
    "        tokenizer.push_to_hub(f\"Mhara/{run_id}\", token=hf_token)\n",
    "        \n",
    "        clean_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36b95f-b3c5-449e-9a30-47abdbbceb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system','content':train_dataset['conversations'][10][0]['content']},\n",
    "    {\"role\" : 'user', 'content' : train_dataset['conversations'][10][1]['content']}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 125,\n",
    "    temperature = 1, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6e922-c60a-4993-8253-a81391da77a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\n",
    "    repo_id=\"Mhara/google_gemma-3-270m-it_ft_ag_news_merged_version\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
