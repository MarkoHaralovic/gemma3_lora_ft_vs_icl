{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f87f3-03fd-4d41-b646-282a3d3e5b2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Time to First Token (TTFT): latency before the first output token is produced.\n",
    "- End-to-End Request Latency : how long it takes from submitting a query to receiving the full response\n",
    "- Time Per Output Token (TPOT): average generation speed in tokens per second. (also known as Inter-token Latency (ITL))\n",
    "- Token Generation Time (TGT): duration from first to last token.\n",
    "- Total Latency: TTFT + TGT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14465cc1-8871-433b-a6e7-3f820107320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-19 19:59:08 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# loosely based on code acquired from https://github.com/rumanxyz/llm-perf-benchmark\n",
    "import unsloth\n",
    "import torch\n",
    "import time\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "import traceback\n",
    "import threading\n",
    "from transformers import TextIteratorStreamer\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from unsloth.chat_templates import get_chat_template \n",
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM,AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f79c37-a8e6-4e56-be8a-55a419fb6dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._dynamo.eval_frame.DisableContext at 0x7faaa94051d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.compiler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d997d1f-0968-4186-8a5d-e0aa310d3ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    def __init__(self, monitoring_interval: float = 0.1, gpu_indices: Optional[List[int]] = None):\n",
    "        self.monitoring_interval = monitoring_interval\n",
    "        self.gpu_indices = gpu_indices if gpu_indices is not None else [0]\n",
    "        self._mem_samples = []\n",
    "        self._util_samples = []\n",
    "        self._is_monitoring = False\n",
    "        self._monitoring_thread = None\n",
    "        self._gpu_memory_usage = []\n",
    "        self._gpu_utilization = []\n",
    "\n",
    "    def start(self):\n",
    "        self._is_monitoring = True\n",
    "        self._mem_samples.clear()\n",
    "        self._util_samples.clear()\n",
    "\n",
    "        def monitor_gpu():\n",
    "            while self._is_monitoring:\n",
    "                try:\n",
    "                    gpus = GPUtil.getGPUs()\n",
    "                    if gpus:\n",
    "                        selected = [g for g in gpus if g.id in self.gpu_indices]\n",
    "\n",
    "                        mem_vals  = [float(g.memoryUsed) for g in selected]       # MB\n",
    "                        util_vals = [float(g.load) * 100.0 for g in selected]     # %\n",
    "\n",
    "                        self._gpu_memory_usage.extend(mem_vals)\n",
    "                        self._gpu_utilization.extend(util_vals)\n",
    "\n",
    "                        self._mem_samples.append(max(mem_vals))\n",
    "                        self._util_samples.append(max(util_vals))\n",
    "\n",
    "                    time.sleep(self.monitoring_interval)\n",
    "                except Exception as e:\n",
    "                    print(f\"GPU monitoring error: {e}\")\n",
    "                    break\n",
    "\n",
    "        self._monitoring_thread = threading.Thread(target=monitor_gpu, daemon=True)\n",
    "        self._monitoring_thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._is_monitoring = False\n",
    "        if self._monitoring_thread:\n",
    "            self._monitoring_thread.join()\n",
    "\n",
    "    def peak_mem(self) -> float:\n",
    "        return max(self._mem_samples) if self._mem_samples else 0.0\n",
    "\n",
    "    def p90_mem(self) -> float:\n",
    "        return float(np.percentile(self._mem_samples, 90)) if self._mem_samples else 0.0\n",
    "\n",
    "    def peak_util(self) -> float:\n",
    "        return max(self._util_samples) if self._util_samples else 0.0\n",
    "\n",
    "    def p90_util(self) -> float:\n",
    "        return float(np.percentile(self._util_samples, 90)) if self._util_samples else 0.0\n",
    "\n",
    "def benchmark_single_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_prompt_text: str,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    top_k: int = 64,\n",
    "    max_new_tokens: int = 100,\n",
    "    do_sample = True,\n",
    "    num_beams : int = None,\n",
    "    c=None,\n",
    "    device: Optional[str] = None,\n",
    "    gpu_indices: Optional[List[int]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    device = str(next(model.parameters()).device) if device is None else device\n",
    "    \n",
    "    gpu_monitor = GPUMonitor(monitoring_interval=0.1, gpu_indices=gpu_indices)\n",
    "    gpu_monitor.start()\n",
    "\n",
    "    start_input_process = time.time()\n",
    "    inputs = tokenizer(input_prompt_text, return_tensors=\"pt\").to(device)\n",
    "    input_process_time = time.time() - start_input_process\n",
    "\n",
    "    generation_kwargs = {\n",
    "        'input_ids': inputs.input_ids,\n",
    "        'attention_mask': inputs.attention_mask,\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'return_dict_in_generate': True,\n",
    "        'output_scores': False\n",
    "    }\n",
    "    \n",
    "    if num_beams is not None and num_beams > 1:\n",
    "        generation_kwargs['num_beams'] = num_beams\n",
    "        generation_kwargs['do_sample'] = False\n",
    "    else:\n",
    "        generation_kwargs['do_sample'] = do_sample\n",
    "        \n",
    "        if do_sample:\n",
    "            if temperature is not None and temperature > 0:\n",
    "                generation_kwargs['temperature'] = temperature\n",
    "            if top_p is not None:\n",
    "                generation_kwargs['top_p'] = top_p\n",
    "            if top_k is not None:\n",
    "                generation_kwargs['top_k'] = top_k\n",
    "\n",
    "    # Streaming generation setup\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=False)\n",
    "    generation_kwargs[\"streamer\"] = streamer\n",
    "\n",
    "    generation_start_time = time.time()\n",
    "    first_token_time = None\n",
    "    result_holder = {}\n",
    "\n",
    "    def _generate():\n",
    "        try:\n",
    "            result_holder[\"out\"] = model.generate(**generation_kwargs)\n",
    "        except Exception as e:\n",
    "            result_holder[\"error\"] = str(e)\n",
    "            import traceback\n",
    "            result_holder[\"traceback\"] = traceback.format_exc()\n",
    "    \n",
    "    print(f\"starting to generate\")\n",
    "    generation_thread = threading.Thread(target=_generate, daemon=True)\n",
    "    generation_thread.start()\n",
    "    print(f\"done generating\")\n",
    "    \n",
    "    try:\n",
    "        for token in streamer:\n",
    "            print(f\"token : {token}\")\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time() - generation_start_time\n",
    "                first_token_start_time = time.time()\n",
    "    except Exception as e:\n",
    "        print(f\"Streamer error: {e}\")\n",
    "        gpu_monitor.stop()\n",
    "        generation_thread.join()\n",
    "        return {}\n",
    "\n",
    "    generation_thread.join()\n",
    "    gpu_monitor.stop()\n",
    "    \n",
    "    print(f\"result_holder : {result_holder}\")\n",
    "    if \"error\" in result_holder:\n",
    "        print(f\"Generation failed: {result_holder['error']}\")\n",
    "        print(result_holder.get('traceback', ''))\n",
    "        return {}\n",
    "\n",
    "    if \"out\" not in result_holder:\n",
    "        print(\"No output generated\")\n",
    "        return {}\n",
    "\n",
    "    total_generation_time = time.time() - generation_start_time\n",
    "    \n",
    "    output = result_holder[\"out\"]\n",
    "    sequences = output.sequences\n",
    "\n",
    "    total_len = int(sequences.shape[1])\n",
    "    input_tokens = inputs.input_ids.shape[1]\n",
    "    output_tokens = max(total_len - input_tokens, 0)\n",
    "    \n",
    "    if output_tokens == 0:\n",
    "        print(\"Warning: No tokens generated\")\n",
    "        return {}\n",
    "\n",
    "    # Safe timing calculations\n",
    "    if first_token_time is None:\n",
    "        first_token_time = 0.001\n",
    "        \n",
    "    decode_time = total_generation_time - first_token_time\n",
    "    if decode_time <= 0:\n",
    "        decode_time = 0.001\n",
    "    \n",
    "    ttft = first_token_time\n",
    "    total_tps = (input_tokens + output_tokens) / total_generation_time if total_generation_time > 0 else 0\n",
    "    decode_tps = output_tokens / decode_time if decode_time > 0 else 0\n",
    "          \n",
    "    # GPU metrics\n",
    "    peak_gpu_usage = gpu_monitor.peak_mem()\n",
    "    p90_gpu_usage = gpu_monitor.p90_mem()\n",
    "    peak_gpu_utilization = gpu_monitor.peak_util()\n",
    "    p90_gpu_utilization = gpu_monitor.p90_util()\n",
    "\n",
    "    benchmark_results = {\n",
    "        'total_generation_time': total_generation_time,\n",
    "        'time_to_first_token_seconds': ttft,\n",
    "        'token_generation_time': decode_time,\n",
    "        'time_per_output_token': 1 / decode_tps if decode_tps > 0 else 0,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': input_tokens + output_tokens,\n",
    "        'tokens_per_second': total_tps,\n",
    "        'output_decode_tokens_per_second': decode_tps,\n",
    "        'input_process_time_seconds': input_process_time,\n",
    "        'e2e_latency': ttft + total_generation_time,\n",
    "        'peak_gpu_memory_mb': peak_gpu_usage,\n",
    "        'p90_gpu_memory_mb': p90_gpu_usage,\n",
    "        'peak_gpu_utilization': peak_gpu_utilization,\n",
    "        'p90_gpu_utilization': p90_gpu_utilization\n",
    "    }\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "    \n",
    "def benchmark_language_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    top_k: int = 64,\n",
    "    max_new_tokens: int = 100,\n",
    "    do_sample = True,\n",
    "    num_beams : int = None,\n",
    "    device: Optional[str] = None,\n",
    "    gpu_indices: Optional[List[int]] = None,\n",
    ") -> Dict[str, Union[float, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Benchmark a language model's performance across multiple prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_results = []\n",
    "    for prompt in prompts:\n",
    "        print(f\"prompt : {prompt}\")\n",
    "        result = benchmark_single_prompt(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            max_new_tokens,\n",
    "            do_sample,\n",
    "            num_beams,\n",
    "            None,\n",
    "            device,\n",
    "            gpu_indices\n",
    "        )\n",
    "        if result:\n",
    "            prompt_results.append(result)\n",
    "\n",
    "    if not prompt_results:\n",
    "        return {}\n",
    "\n",
    "    ttft_list = [result['time_to_first_token_seconds'] for result in prompt_results]\n",
    "    tpot_list = [result['time_per_output_token'] for result in prompt_results]\n",
    "    tgt_list = [result['total_generation_time'] for result in prompt_results]\n",
    "    e2e_latency_list = [result['e2e_latency'] for result in prompt_results]\n",
    "    decode_tps_list = [result['output_decode_tokens_per_second'] for result in prompt_results]\n",
    "    gpu_usage_list = [result['peak_gpu_memory_mb'] for result in prompt_results]\n",
    "    gpu_util_list = [result['peak_gpu_utilization'] for result in prompt_results]\n",
    "\n",
    "    # Aggregate metrics\n",
    "    aggregate_results = {\n",
    "        # Time to First Token (TTFT) metrics\n",
    "        'p50_ttft_seconds': round(np.percentile(ttft_list, 50), 3),\n",
    "        'p90_ttft_seconds': round(np.percentile(ttft_list, 90), 3),\n",
    "\n",
    "        # Time per output token (TPOT) metrics\n",
    "        'p50_tpot_seconds': round(np.percentile(tpot_list, 50), 3),\n",
    "        'p90_tpot_seconds': round(np.percentile(tpot_list, 90), 3),\n",
    "\n",
    "        # Total generation time (TGT) metrics\n",
    "        'p50_tgt_seconds': round(np.percentile(tgt_list, 50), 3),\n",
    "        'p90_tgt_seconds': round(np.percentile(tgt_list, 90), 3),\n",
    "\n",
    "        # End to end latency (e2e latency) metrics\n",
    "        'p50_e2elatency_seconds': round(np.percentile(e2e_latency_list, 50), 3),\n",
    "        'p90_e2elatency_seconds': round(np.percentile(e2e_latency_list, 90), 3),\n",
    "\n",
    "        # Output Decode Tokens Per Second metrics\n",
    "        'p50_decode_tps': round(np.percentile(decode_tps_list, 50), 3),\n",
    "        'p90_decode_tps': round(np.percentile(decode_tps_list, 90), 3),\n",
    "\n",
    "        # GPU Memory Usage metrics\n",
    "        'max_gpu_memory_mb': round(max(gpu_usage_list), 3),\n",
    "        'p90_gpu_memory_mb': round(np.percentile(gpu_usage_list, 90), 3),\n",
    "\n",
    "        # GPU Utilization metrics\n",
    "        'max_gpu_utilization': round(max(gpu_util_list), 3),\n",
    "        'p90_gpu_utilization': round(np.percentile(gpu_util_list, 90), 3)\n",
    "    }\n",
    "\n",
    "    return aggregate_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b050d60-b1b2-42a5-9c1b-c9c2700fdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_variants = {\n",
    "    \"k_shot\": [0, 5, 10, 25],\n",
    "    \"decoding_strategy\": {\n",
    "        \"default\": {\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 64,\n",
    "            \"max_gen_toks\": 125,\n",
    "            \"do_sample\": True,\n",
    "            \"num_beams\": None\n",
    "        },\n",
    "        \"greedy\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": None,\n",
    "            \"top_k\": None,\n",
    "            \"do_sample\": False,\n",
    "            \"max_gen_toks\": 125,\n",
    "            \"num_beams\": None\n",
    "        },\n",
    "        \"beam\": {\n",
    "            \"num_beams\": 5,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": None,\n",
    "            \"top_k\": None,\n",
    "            \"do_sample\": False,\n",
    "            \"max_gen_toks\": 125\n",
    "        },\n",
    "        \"top_p\": {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": None,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_gen_toks\": 125,\n",
    "            \"num_beams\": None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d3a286-c65e-455b-98ce-d14bec48f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prompts(n_shot: int, task_type: str) -> list:\n",
    "    if task_type == \"classification\":\n",
    "        base_prompt = \"Classify the following news article into one of these categories: World, Sports, Business, Science/Technology.\"\n",
    "        examples = \"\"\n",
    "        sample_examples = [\n",
    "            (\"The stock market closed higher today as investors showed optimism about the latest tech earnings.\", \"Business\"),\n",
    "            (\"The football team secured a 3-1 victory in the championship game last night.\", \"Sports\"),\n",
    "            (\"NASA announced a new mission to explore the outer reaches of our solar system.\", \"Science/Technology\"),\n",
    "            (\"World leaders met today to discuss climate change policies at the United Nations.\", \"World\")\n",
    "        ]\n",
    "\n",
    "        for i in range(min(n_shot, len(sample_examples))):\n",
    "            text, label = sample_examples[i]\n",
    "            examples += f\"\\nExample {i+1}:\\nText: {text}\\nCategory: {label}\\n\"\n",
    "\n",
    "        query = \"\\nText: The national basketball league announced the start of the new season.\\nCategory:\"\n",
    "        return [base_prompt + examples + query]\n",
    "\n",
    "    elif task_type == \"qa\":\n",
    "        base_prompt = \"Read the passage and answer the question. If the answer is not in the passage, say 'unanswerable'.\"\n",
    "        examples = \"\"\n",
    "        sample_examples = [\n",
    "            (\n",
    "                \"Context: Machine learning is a subfield of artificial intelligence that focuses on enabling systems to learn from data without being explicitly programmed.\",\n",
    "                \"What does machine learning focus on?\",\n",
    "                \"enabling systems to learn from data without being explicitly programmed.\"\n",
    "            ),\n",
    "            (\n",
    "                \"Context: The Eiffel Tower is located in Paris, France, and was completed in 1889.\",\n",
    "                \"Where is the Statue of Liberty located?\",\n",
    "                \"unanswerable\"\n",
    "            ),\n",
    "            (\n",
    "                \"Context: Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "                \"At what temperature does water boil?\",\n",
    "                \"100 degrees Celsius.\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        for i in range(min(n_shot, len(sample_examples))):\n",
    "            context, question, answer = sample_examples[i]\n",
    "            examples += f\"\\nExample {i+1}:\\n{context}\\nQuestion: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "        # New QA pair for model\n",
    "        query = (\n",
    "            \"\\nContext: Artificial intelligence enables computers to perform tasks that typically require human intelligence, such as visual perception and language understanding.\\n\"\n",
    "            \"Question: What is artificial intelligence?\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        return [base_prompt + examples + query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b68b87-c168-433b-840e-fbca2643fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADERS = [\n",
    "    'timestamp', 'model_id', 'adapter_id', 'task_type', 'n_shot', 'decoding_strategy',\n",
    "    'temperature', 'top_p', 'top_k', 'do_sample', 'num_beams', 'max_gen_toks',\n",
    "    'p50_ttft_seconds', 'p90_ttft_seconds', 'p50_tpot_seconds', 'p90_tpot_seconds',\n",
    "    'p50_tgt_seconds', 'p90_tgt_seconds', 'p50_e2elatency_seconds', 'p90_e2elatency_seconds',\n",
    "    'p50_decode_tps', 'p90_decode_tps', 'max_gpu_memory_mb', 'p90_gpu_memory_mb',\n",
    "    'max_gpu_utilization', 'p90_gpu_utilization'\n",
    "]\n",
    "\n",
    "def initialize_csv(csv_path: str):\n",
    "    if not os.path.exists(csv_path):\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=CSV_HEADERS)\n",
    "            writer.writeheader()\n",
    "\n",
    "def append_results_to_csv(csv_path: str, row_data: Dict[str, Any]):\n",
    "    flat_data = {k: v for k, v in row_data.items() if k != 'metrics'}\n",
    "    flat_data.update(row_data.get('metrics', {}))\n",
    "    \n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=CSV_HEADERS)\n",
    "        writer.writerow(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cefa16-a282-4617-92ec-7475d4a6396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma3_lora_adapters  = {\n",
    "    \"google/gemma-3-270m-it\" :  {\n",
    "        \"classification\" : \"Mhara/google_gemma-3-270m-it_ft_ag_news_v3\",\n",
    "        \"question_answering\" : \"Mhara/google_gemma-3-270m-it_ft_squad_v2\"\n",
    "    },\n",
    "    \"google/gemma-3-1b-it\" :  {\n",
    "        \"classification\" : \"Mhara/google_gemma-3-1b-it_ft_ag_news\",\n",
    "        \"question_answering\" : \"Mhara/google_gemma-3-1b-it_ft_squad_v2\"\n",
    "    },\n",
    "\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b1212-215d-4466-9c2f-8a0c76c292a9",
   "metadata": {},
   "source": [
    "    \"google/gemma-3-4b-it\" :  {\n",
    "        \"classification\" : \"Mhara/google_gemma-3-1b-it_ft_ag_news_v2\",\n",
    "        \"question_answering\" : \"Mhara/google_gemma-3-4b-it_ft_squad_v2\"\n",
    "    },\n",
    "    \"google/gemma-3-12b-it\" :  {\n",
    "        \"classification\" : \"Mhara/google_gemma-3-12b-it_ft_ag_news\",\n",
    "        \"question_answering\" : \"Mhara/google_gemma-3-12b-it_ft_squad_v2\"\n",
    "    },\n",
    "    \"google/gemma-3-27b-it\" :  {\n",
    "        \"classification\" : \"Mhara/google_gemma-3-27b-it_ft_ag_news\",\n",
    "        \"question_answering\" : \"Mhara/google_gemma-3-27b-it_ft_squad_v2\"\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042ba7b3-a5af-4621-93ca-cfb014936696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up vLLM and CUDA contexts\n"
     ]
    }
   ],
   "source": [
    "def clean_gpu():\n",
    "    import os\n",
    "    os.system(\"\"\"\n",
    "    echo \"Cleaning up vLLM and CUDA contexts\"\n",
    "    pkill -f \"vllm\" || true\n",
    "    pkill -f \"engine_core\" || true\n",
    "    pkill -f \"torchrun\" || true\n",
    "    sleep 2\n",
    "    fuser -k /dev/nvidia* || true\n",
    "    \"\"\")\n",
    "clean_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d109edbd-3107-4569-af4e-bf9e35c7225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(base_model_id, adapter_id):\n",
    "    cfg = PeftConfig.from_pretrained(adapter_id)\n",
    "    base_id = cfg.base_model_name_or_path or base_model_id\n",
    "\n",
    "    _tok = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    _model = PeftModel.from_pretrained(base, adapter_id)\n",
    "    _model.eval()\n",
    "    tok = get_chat_template(_tok, chat_template=\"gemma3\")\n",
    "\n",
    "    return _model, _tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162ded84-1d8b-46c2-80d7-efb13bcee1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_IND = 7\n",
    "device = f\"cuda:{CUDA_DEVICE_IND}\"\n",
    "gpu_indices = [CUDA_DEVICE_IND ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a92f8b-8528-47d7-9516-073e5500d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_path = \"results/benchmark_results.csv\"\n",
    "initialize_csv(csv_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da90c41-aaee-4055-8f49-c796a27e1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "clean_gpu()\n",
    "        \n",
    "for model_id, adapters in gemma3_lora_adapters.items():\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        base_model.eval()\n",
    "\n",
    "        for n_shot in icl_variants[\"k_shot\"]:\n",
    "            for ds_name, ds_config in icl_variants[\"decoding_strategy\"].items():\n",
    "                print(f\"\\n  Base model - {n_shot}-shot, {ds_name} decoding\")\n",
    "\n",
    "                prompts = get_sample_prompts(n_shot, \"classification\")\n",
    "\n",
    "                try:\n",
    "                    result = benchmark_language_model(\n",
    "                            base_model,\n",
    "                            tokenizer,\n",
    "                            prompts,\n",
    "                            temperature=ds_config.get(\"temperature\", 1.0),\n",
    "                            top_p=ds_config.get(\"top_p\", 0.95),\n",
    "                            top_k=ds_config.get(\"top_k\", 64),\n",
    "                            max_new_tokens=ds_config.get(\"max_gen_toks\", 125),\n",
    "                            do_sample=ds_config.get(\"do_sample\", True),\n",
    "                            num_beams=ds_config.get(\"num_beams\"),\n",
    "                            device=device,\n",
    "                            gpu_indices=gpu_indices\n",
    "                        )\n",
    "                                        \n",
    "                    row_data = {\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'model_id': model_id,\n",
    "                        'adapter_id': 'base_model',\n",
    "                        'task_type': 'general',\n",
    "                        'n_shot': n_shot,\n",
    "                        'decoding_strategy': ds_name,\n",
    "                        'temperature': ds_config.get(\"temperature\"),\n",
    "                        'top_p': ds_config.get(\"top_p\"),\n",
    "                        'top_k': ds_config.get(\"top_k\"),\n",
    "                        'do_sample': ds_config.get(\"do_sample\"),\n",
    "                        'num_beams': ds_config.get(\"num_beams\"),\n",
    "                        'max_gen_toks': ds_config.get(\"max_gen_toks\"),\n",
    "                        'metrics': result\n",
    "                    }\n",
    "\n",
    "                    append_results_to_csv(csv_output_path, row_data)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "        del base_model\n",
    "        torch.cuda.empty_cache()\n",
    "        clean_gpu()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    for task_type, adapter_id in adapters.items():\n",
    "        try:\n",
    "            model, tokenizer = load_adapter(model_id, adapter_id)\n",
    "            for n_shot in icl_variants[\"k_shot\"]:\n",
    "                for ds_name, ds_config in icl_variants[\"decoding_strategy\"].items():\n",
    "                    print(f\"\\n  {task_type} - {n_shot}-shot, {ds_name} decoding\")\n",
    "\n",
    "                    prompts = get_sample_prompts(n_shot, task_type)\n",
    "\n",
    "                    try:\n",
    "                        result = benchmark_language_model(\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            prompts,\n",
    "                            temperature=ds_config.get(\"temperature\", 1.0),\n",
    "                            top_p=ds_config.get(\"top_p\", 0.95),\n",
    "                            top_k=ds_config.get(\"top_k\", 64),\n",
    "                            max_new_tokens=ds_config.get(\"max_gen_toks\", 125),\n",
    "                            do_sample=ds_config.get(\"do_sample\", True),\n",
    "                            num_beams=ds_config.get(\"num_beams\"),\n",
    "                            device=device,\n",
    "                            gpu_indices=gpu_indices\n",
    "                        )\n",
    "\n",
    "                        row_data = {\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                            'model_id': model_id,\n",
    "                            'adapter_id': adapter_id,\n",
    "                            'task_type': task_type,\n",
    "                            'n_shot': n_shot,\n",
    "                            'decoding_strategy': ds_name,\n",
    "                            'temperature': ds_config.get(\"temperature\"),\n",
    "                            'top_p': ds_config.get(\"top_p\"),\n",
    "                            'top_k': ds_config.get(\"top_k\"),\n",
    "                            'do_sample': ds_config.get(\"do_sample\"),\n",
    "                            'num_beams': ds_config.get(\"num_beams\"),\n",
    "                            'max_gen_toks': ds_config.get(\"max_gen_toks\"),\n",
    "                            'metrics': result\n",
    "                        }\n",
    "\n",
    "                        append_results_to_csv(csv_output_path, row_data)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            clean_gpu()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
